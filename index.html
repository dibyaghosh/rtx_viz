
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>RTX Dataset Viewer</title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
  </head>

<body>
<div class="container">
<nav class="navbar sticky-top bg-body-tertiary">
  <div class="container-fluid">
    <a class="navbar-brand" href="#">RTX Datasets</a>
  </div>
  <ul class="nav nav-tabs">
  <li class="nav-item dropdown">
    <a class="nav-link dropdown-toggle" data-bs-toggle="dropdown" href="#" role="button" aria-expanded="false">Datasets</a>
    <ul class="dropdown-menu">
        <li> <a class='dropdown-item' href='#fractal20220817_data'> fractal20220817_data </a> </li>
<li> <a class='dropdown-item' href='#kuka'> kuka </a> </li>
<li> <a class='dropdown-item' href='#bridge'> bridge </a> </li>
<li> <a class='dropdown-item' href='#taco_play'> taco_play </a> </li>
<li> <a class='dropdown-item' href='#jaco_play'> jaco_play </a> </li>
<li> <a class='dropdown-item' href='#berkeley_cable_routing'> berkeley_cable_routing </a> </li>
<li> <a class='dropdown-item' href='#roboturk'> roboturk </a> </li>
<li> <a class='dropdown-item' href='#nyu_door_opening_surprising_effectiveness'> nyu_door_opening_surprising_effectiveness </a> </li>
<li> <a class='dropdown-item' href='#viola'> viola </a> </li>
<li> <a class='dropdown-item' href='#berkeley_autolab_ur5'> berkeley_autolab_ur5 </a> </li>
<li> <a class='dropdown-item' href='#toto'> toto </a> </li>
<li> <a class='dropdown-item' href='#language_table'> language_table </a> </li>
<li> <a class='dropdown-item' href='#columbia_cairlab_pusht_real'> columbia_cairlab_pusht_real </a> </li>
<li> <a class='dropdown-item' href='#stanford_kuka_multimodal_dataset_converted_externally_to_rlds'> stanford_kuka_multimodal_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#nyu_rot_dataset_converted_externally_to_rlds'> nyu_rot_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#stanford_hydra_dataset_converted_externally_to_rlds'> stanford_hydra_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#austin_buds_dataset_converted_externally_to_rlds'> austin_buds_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#nyu_franka_play_dataset_converted_externally_to_rlds'> nyu_franka_play_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#maniskill_dataset_converted_externally_to_rlds'> maniskill_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#cmu_franka_exploration_dataset_converted_externally_to_rlds'> cmu_franka_exploration_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#ucsd_kitchen_dataset_converted_externally_to_rlds'> ucsd_kitchen_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#ucsd_pick_and_place_dataset_converted_externally_to_rlds'> ucsd_pick_and_place_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#austin_sailor_dataset_converted_externally_to_rlds'> austin_sailor_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#austin_sirius_dataset_converted_externally_to_rlds'> austin_sirius_dataset_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#bc_z'> bc_z </a> </li>
<li> <a class='dropdown-item' href='#usc_cloth_sim_converted_externally_to_rlds'> usc_cloth_sim_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#utokyo_pr2_opening_fridge_converted_externally_to_rlds'> utokyo_pr2_opening_fridge_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds'> utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#utokyo_saytap_converted_externally_to_rlds'> utokyo_saytap_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#utokyo_xarm_pick_and_place_converted_externally_to_rlds'> utokyo_xarm_pick_and_place_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#utokyo_xarm_bimanual_converted_externally_to_rlds'> utokyo_xarm_bimanual_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#robo_net'> robo_net </a> </li>
<li> <a class='dropdown-item' href='#berkeley_mvp_converted_externally_to_rlds'> berkeley_mvp_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#berkeley_rpt_converted_externally_to_rlds'> berkeley_rpt_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#kaist_nonprehensile_converted_externally_to_rlds'> kaist_nonprehensile_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#stanford_mask_vit_converted_externally_to_rlds'> stanford_mask_vit_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#tokyo_u_lsmo_converted_externally_to_rlds'> tokyo_u_lsmo_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#dlr_sara_pour_converted_externally_to_rlds'> dlr_sara_pour_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#dlr_sara_grid_clamp_converted_externally_to_rlds'> dlr_sara_grid_clamp_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#dlr_edan_shared_control_converted_externally_to_rlds'> dlr_edan_shared_control_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#asu_table_top_converted_externally_to_rlds'> asu_table_top_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#stanford_robocook_converted_externally_to_rlds'> stanford_robocook_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#eth_agent_affordances'> eth_agent_affordances </a> </li>
<li> <a class='dropdown-item' href='#imperialcollege_sawyer_wrist_cam'> imperialcollege_sawyer_wrist_cam </a> </li>
<li> <a class='dropdown-item' href='#iamlab_cmu_pickup_insert_converted_externally_to_rlds'> iamlab_cmu_pickup_insert_converted_externally_to_rlds </a> </li>
<li> <a class='dropdown-item' href='#uiuc_d3field'> uiuc_d3field </a> </li>
<li> <a class='dropdown-item' href='#utaustin_mutex'> utaustin_mutex </a> </li>
<li> <a class='dropdown-item' href='#berkeley_fanuc_manipulation'> berkeley_fanuc_manipulation </a> </li>
<li> <a class='dropdown-item' href='#cmu_play_fusion'> cmu_play_fusion </a> </li>
<li> <a class='dropdown-item' href='#cmu_stretch'> cmu_stretch </a> </li>
<li> <a class='dropdown-item' href='#berkeley_gnm_recon'> berkeley_gnm_recon </a> </li>
<li> <a class='dropdown-item' href='#berkeley_gnm_cory_hall'> berkeley_gnm_cory_hall </a> </li>
<li> <a class='dropdown-item' href='#berkeley_gnm_sac_son'> berkeley_gnm_sac_son </a> </li>
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link disabled" aria-disabled="true">Made by Dibya</a>
  </li>
</ul>
</nav>

    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="fractal20220817_data"> fractal20220817_data 
    <small class="text-body-secondary">(87212 trajs, 111.07 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (256, 320, 3) </th></tr>
<tr><td> place green can upright </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="fractal20220817_data/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick apple from white bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="fractal20220817_data/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> move blue chip bag near water bottle </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="fractal20220817_data/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_fractal20220817_data" role="button" aria-expanded="false" aria-controls="collapse_fractal20220817_data">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_fractal20220817_data">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='fractal20220817_data',
    full_name='fractal20220817_data/0.1.0',
    description="""
    Table-top manipulation with 17 objects
    """,
    homepage='https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html',
    data_path='gs://gresearch/robotics/fractal20220817_data/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=111.07 GiB,
    features=FeaturesDict({
        'aspects': FeaturesDict({
            'already_success': bool,
            'feasible': bool,
            'has_aspects': bool,
            'success': bool,
            'undesirable': bool,
        }),
        'attributes': FeaturesDict({
            'collection_mode': int64,
            'collection_mode_name': string,
            'data_type': int64,
            'data_type_name': string,
            'env': int64,
            'env_name': string,
            'location': int64,
            'location_name': string,
            'objects_family': int64,
            'objects_family_name': string,
            'task_family': int64,
            'task_family_name': string,
        }),
        'steps': Dataset({
            'action': FeaturesDict({
                'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
                'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
                'gripper_closed': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_commanded': Tensor(shape=(1,), dtype=float32),
                'height_to_bottom': Tensor(shape=(1,), dtype=float32),
                'image': Image(shape=(256, 320, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'orientation_box': Tensor(shape=(2, 3), dtype=float32),
                'orientation_start': Tensor(shape=(4,), dtype=float32),
                'robot_orientation_positions_box': Tensor(shape=(3, 3), dtype=float32),
                'rotation_delta_to_go': Tensor(shape=(3,), dtype=float32),
                'src_rotation': Tensor(shape=(4,), dtype=float32),
                'vector_to_go': Tensor(shape=(3,), dtype=float32),
                'workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=87212, num_shards=1024>,
    },
    citation="""@article{brohan2022rt,
      title={Rt-1: Robotics transformer for real-world control at scale},
      author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
      journal={arXiv preprint arXiv:2212.06817},
      year={2022}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="kuka"> kuka 
    <small class="text-body-secondary">(580392 trajs, 778.02 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (512, 640, 3) </th></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kuka/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kuka/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kuka/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_kuka" role="button" aria-expanded="false" aria-controls="collapse_kuka">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_kuka">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='kuka',
    full_name='kuka/0.1.0',
    description="""
    Bin picking and rearrangement tasks
    """,
    homepage='https://arxiv.org/abs/1806.10293',
    data_path='gs://gresearch/robotics/kuka/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=778.02 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
                'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'clip_function_input/base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
                'clip_function_input/workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
                'gripper_closed': Tensor(shape=(1,), dtype=float32),
                'height_to_bottom': Tensor(shape=(1,), dtype=float32),
                'image': Image(shape=(512, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'task_id': Tensor(shape=(1,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
        'success': bool,
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=580392, num_shards=1024>,
    },
    citation="""@article{kalashnikov2018qt,
      title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
      author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
      journal={arXiv preprint arXiv:1806.10293},
      year={2018}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="bridge"> bridge 
    <small class="text-body-secondary">(25460 trajs, 387.49 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (480, 640, 3) </th></tr>
<tr><td> PICK UP THE SPOON AND PUT NEAR THE VESSEL </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bridge/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> put fork from basket to tray </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bridge/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> Place the red vegetable in the silver pot. </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bridge/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_bridge" role="button" aria-expanded="false" aria-controls="collapse_bridge">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_bridge">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='bridge',
    full_name='bridge/0.1.0',
    description="""
    WidowX interacting with toy kitchens
    """,
    homepage='https://rail-berkeley.github.io/bridgedata/',
    data_path='gs://gresearch/robotics/bridge/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=387.49 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'open_gripper': bool,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=3475, num_shards=512>,
        'train': <SplitInfo num_examples=25460, num_shards=1024>,
    },
    citation="""@inproceedings{walke2023bridgedata,
        title={BridgeData V2: A Dataset for Robot Learning at Scale},
        author={Walke, Homer and Black, Kevin and Lee, Abraham and Kim, Moo Jin and Du, Max and Zheng, Chongyi and Zhao, Tony and Hansen-Estruch, Philippe and Vuong, Quan and He, Andre and Myers, Vivek and Fang, Kuan and Finn, Chelsea and Levine, Sergey},
        booktitle={Conference on Robot Learning (CoRL)},
        year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="taco_play"> taco_play 
    <small class="text-body-secondary">(3242 trajs, 47.77 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> structured_language_instruction </th><th> natural_language_instruction </th><th> rgb_gripper (84, 84, 3) </th><th> rgb_static (150, 200, 3) </th></tr>
<tr><td> place_yellow_left_cabinet </td><td> put the yellow object inside the left cabinet </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode0_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode0_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place_pink_box </td><td> move to the box and place the pink object </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode1_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode1_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> rotate_yellow_block_left </td><td> grasp the yellow block and turn it left </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode2_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="taco_play/episode2_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_taco_play" role="button" aria-expanded="false" aria-controls="collapse_taco_play">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_taco_play">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='taco_play',
    full_name='taco_play/0.1.0',
    description="""
    Franka arm interacting with kitchen
    """,
    homepage='https://www.kaggle.com/datasets/oiermees/taco-robot',
    data_path='gs://gresearch/robotics/taco_play/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=47.77 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'actions': Tensor(shape=(7,), dtype=float32),
                'rel_actions_gripper': Tensor(shape=(7,), dtype=float32),
                'rel_actions_world': Tensor(shape=(7,), dtype=float32),
                'terminate_episode': float32,
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'depth_gripper': Tensor(shape=(84, 84), dtype=float32),
                'depth_static': Tensor(shape=(150, 200), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'rgb_gripper': Image(shape=(84, 84, 3), dtype=uint8),
                'rgb_static': Image(shape=(150, 200, 3), dtype=uint8),
                'robot_obs': Tensor(shape=(15,), dtype=float32),
                'structured_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=361, num_shards=64>,
        'train': <SplitInfo num_examples=3242, num_shards=511>,
    },
    citation="""@inproceedings{rosete2022tacorl,
    author = {Erick Rosete-Beas and Oier Mees and Gabriel Kalweit and Joschka Boedecker and Wolfram Burgard},
    title = {Latent Plans for Task Agnostic Offline Reinforcement Learning},
    journal = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
    year = {2022}
    }
    @inproceedings{mees23hulc2,
    title={Grounding  Language  with  Visual  Affordances  over  Unstructured  Data},
    author={Oier Mees and Jessica Borja-Diaz and Wolfram Burgard},
    booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    year={2023},
    address = {London, UK}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="jaco_play"> jaco_play 
    <small class="text-body-secondary">(976 trajs, 9.24 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (224, 224, 3) </th><th> image_wrist (224, 224, 3) </th></tr>
<tr><td> pick up the milk dairy </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode0_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the black bowl in the oven </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode1_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick up the green cup </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="jaco_play/episode2_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_jaco_play" role="button" aria-expanded="false" aria-controls="collapse_jaco_play">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_jaco_play">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='jaco_play',
    full_name='jaco_play/0.1.0',
    description="""
    Jaco 2 pick place on table top
    """,
    homepage='https://github.com/clvrai/clvr_jaco_play_dataset',
    data_path='gs://gresearch/robotics/jaco_play/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=9.24 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'end_effector_cartesian_pos': Tensor(shape=(7,), dtype=float32),
                'end_effector_cartesian_velocity': Tensor(shape=(6,), dtype=float32),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'image_wrist': Image(shape=(224, 224, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(8,), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=109, num_shards=8>,
        'train': <SplitInfo num_examples=976, num_shards=128>,
    },
    citation="""@software{dass2023jacoplay,
      author = {Dass, Shivin and Yapeter, Jullian and Zhang, Jesse and Zhang, Jiahui
                and Pertsch, Karl and Nikolaidis, Stefanos and Lim, Joseph J.},
      title = {CLVR Jaco Play Dataset},
      url = {https://github.com/clvrai/clvr_jaco_play_dataset},
      version = {1.0.0},
      year = {2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_cable_routing"> berkeley_cable_routing 
    <small class="text-body-secondary">(1482 trajs, 4.67 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> wrist45_image (128, 128, 3) </th><th> image (128, 128, 3) </th><th> wrist225_image (128, 128, 3) </th><th> top_image (128, 128, 3) </th></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode0_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode0_wrist225_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode0_top_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode1_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode1_wrist225_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode1_top_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode2_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode2_wrist225_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_cable_routing/episode2_top_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_cable_routing" role="button" aria-expanded="false" aria-controls="collapse_berkeley_cable_routing">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_cable_routing">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_cable_routing',
    full_name='berkeley_cable_routing/0.1.0',
    description="""
    Routing cable into clamps on table top
    """,
    homepage='https://sites.google.com/view/cablerouting/home',
    data_path='gs://gresearch/robotics/berkeley_cable_routing/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=4.67 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(7,), dtype=float32),
                'top_image': Image(shape=(128, 128, 3), dtype=uint8),
                'wrist225_image': Image(shape=(128, 128, 3), dtype=uint8),
                'wrist45_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=165, num_shards=4>,
        'train': <SplitInfo num_examples=1482, num_shards=64>,
    },
    citation="""@article{luo2023multistage,
      author    = {Jianlan Luo and Charles Xu and Xinyang Geng and Gilbert Feng and Kuan Fang and Liam Tan and Stefan Schaal and Sergey Levine},
      title     = {Multi-Stage Cable Routing through Hierarchical Imitation Learning},
      journal   = {arXiv pre-print},
      year      = {2023},
      url       = {https://arxiv.org/abs/2307.08927},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="roboturk"> roboturk 
    <small class="text-body-secondary">(1796 trajs, 45.39 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> front_rgb (480, 640, 3) </th></tr>
<tr><td> object search </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="roboturk/episode0_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> object search </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="roboturk/episode1_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> layout laundry </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="roboturk/episode2_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_roboturk" role="button" aria-expanded="false" aria-controls="collapse_roboturk">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_roboturk">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='roboturk',
    full_name='roboturk/0.1.0',
    description="""
    Cloth folding, bowl stacking
    """,
    homepage='https://roboturk.stanford.edu/dataset_real.html',
    data_path='gs://gresearch/robotics/roboturk/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=45.39 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'front_rgb': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=199, num_shards=60>,
        'train': <SplitInfo num_examples=1796, num_shards=494>,
    },
    citation="""@inproceedings{mandlekar2019scaling,
              title={Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity},
              author={Mandlekar, Ajay and Booher, Jonathan and Spero, Max and Tung, Albert and Gupta, Anchit and Zhu, Yuke and Garg, Animesh and Savarese, Silvio and Fei-Fei, Li},
              booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
              pages={1048--1055},
              year={2019},
              organization={IEEE}
            }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="nyu_door_opening_surprising_effectiveness"> nyu_door_opening_surprising_effectiveness 
    <small class="text-body-secondary">(435 trajs, 7.12 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (720, 960, 3) </th></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_door_opening_surprising_effectiveness/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_door_opening_surprising_effectiveness/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_door_opening_surprising_effectiveness/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_nyu_door_opening_surprising_effectiveness" role="button" aria-expanded="false" aria-controls="collapse_nyu_door_opening_surprising_effectiveness">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_nyu_door_opening_surprising_effectiveness">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_door_opening_surprising_effectiveness',
    full_name='nyu_door_opening_surprising_effectiveness/0.1.0',
    description="""
    Hello robot opening cabinets, microwaves etc
    """,
    homepage='https://jyopari.github.io/VINN/',
    data_path='gs://gresearch/robotics/nyu_door_opening_surprising_effectiveness/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=7.12 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(720, 960, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=49, num_shards=8>,
        'train': <SplitInfo num_examples=435, num_shards=64>,
    },
    citation="""@misc{pari2021surprising,
        title={The Surprising Effectiveness of Representation Learning for Visual Imitation}, 
        author={Jyothish Pari and Nur Muhammad Shafiullah and Sridhar Pandian Arunachalam and Lerrel Pinto},
        year={2021},
        eprint={2112.01511},
        archivePrefix={arXiv},
        primaryClass={cs.RO}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="viola"> viola 
    <small class="text-body-secondary">(135 trajs, 10.40 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> eye_in_hand_rgb (224, 224, 3) </th><th> agentview_rgb (224, 224, 3) </th></tr>
<tr><td> arrange plate and fork </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode0_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode0_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> arrange plate and fork </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode1_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode1_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> make coffee </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode2_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="viola/episode2_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_viola" role="button" aria-expanded="false" aria-controls="collapse_viola">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_viola">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='viola',
    full_name='viola/0.1.0',
    description="""
    Franka robot interacting with stylized kitchen tasks
    """,
    homepage='https://ut-austin-rpl.github.io/VIOLA/',
    data_path='gs://gresearch/robotics/viola/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=10.40 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'agentview_rgb': Image(shape=(224, 224, 3), dtype=uint8),
                'ee_states': Tensor(shape=(16,), dtype=float32),
                'eye_in_hand_rgb': Image(shape=(224, 224, 3), dtype=uint8),
                'gripper_states': Tensor(shape=(1,), dtype=float32),
                'joint_states': Tensor(shape=(7,), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=15, num_shards=7>,
        'train': <SplitInfo num_examples=135, num_shards=81>,
    },
    citation="""@article{zhu2022viola,
      title={VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors},
      author={Zhu, Yifeng and Joshi, Abhishek and Stone, Peter and Zhu, Yuke},
      journal={6th Annual Conference on Robot Learning (CoRL)},
      year={2022}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_autolab_ur5"> berkeley_autolab_ur5 
    <small class="text-body-secondary">(896 trajs, 76.39 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (480, 640, 3) </th><th> hand_image (480, 640, 3) </th></tr>
<tr><td> pick up the blue cup and put it into the brown cup.  </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> put the ranch bottle into the pot </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> put the ranch bottle into the pot </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_autolab_ur5/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_autolab_ur5" role="button" aria-expanded="false" aria-controls="collapse_berkeley_autolab_ur5">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_autolab_ur5">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_autolab_ur5',
    full_name='berkeley_autolab_ur5/0.1.0',
    description="""
    UR5 performing cloth manipulation, pick place etc tasks
    """,
    homepage='https://sites.google.com/view/berkeley-ur5/home',
    data_path='gs://gresearch/robotics/berkeley_autolab_ur5/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=76.39 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'image_with_depth': Image(shape=(480, 640, 1), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(15,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=104, num_shards=50>,
        'train': <SplitInfo num_examples=896, num_shards=412>,
    },
    citation="""@misc{BerkeleyUR5Website,
      title = {Berkeley {UR5} Demonstration Dataset},
      author = {Lawrence Yunliang Chen and Simeon Adebola and Ken Goldberg},
      howpublished = {https://sites.google.com/view/berkeley-ur5/home},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="toto"> toto 
    <small class="text-body-secondary">(902 trajs, 127.66 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (480, 640, 3) </th></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="toto/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="toto/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="toto/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_toto" role="button" aria-expanded="false" aria-controls="collapse_toto">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_toto">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='toto',
    full_name='toto/0.1.0',
    description="""
    Franka scooping and pouring tasks
    """,
    homepage='https://toto-benchmark.org/',
    data_path='gs://gresearch/robotics/toto/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=127.66 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'open_gripper': bool,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=101, num_shards=52>,
        'train': <SplitInfo num_examples=902, num_shards=415>,
    },
    citation="""@inproceedings{zhou2023train,
      author={Zhou, Gaoyue and Dean, Victoria and Srirama, Mohan Kumar and Rajeswaran, Aravind and Pari, Jyothish and Hatch, Kyle and Jain, Aryan and Yu, Tianhe and Abbeel, Pieter and Pinto, Lerrel and Finn, Chelsea and Gupta, Abhinav},
      booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
      title={Train Offline, Test Online: A Real Robot Learning Benchmark}, 
      year={2023},
     }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="language_table"> language_table 
    <small class="text-body-secondary">(442226 trajs, 399.23 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> rgb (360, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="language_table/episode0_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="language_table/episode1_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="language_table/episode2_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_language_table" role="button" aria-expanded="false" aria-controls="collapse_language_table">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_language_table">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='language_table',
    full_name='language_table/0.0.1',
    description="""
    
    """,
    homepage='https://www.tensorflow.org/datasets/catalog/language_table',
    data_path='gs://gresearch/robotics/language_table/0.0.1',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=399.23 GiB,
    features=FeaturesDict({
        'episode_id': string,
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'effector_target_translation': Tensor(shape=(2,), dtype=float32),
                'effector_translation': Tensor(shape=(2,), dtype=float32),
                'instruction': Tensor(shape=(512,), dtype=int32),
                'rgb': Image(shape=(360, 640, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=442226, num_shards=500>,
    },
    citation="""""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="columbia_cairlab_pusht_real"> columbia_cairlab_pusht_real 
    <small class="text-body-secondary">(122 trajs, 2.80 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> wrist_image (240, 320, 3) </th><th> image (240, 320, 3) </th></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="columbia_cairlab_pusht_real/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_columbia_cairlab_pusht_real" role="button" aria-expanded="false" aria-controls="collapse_columbia_cairlab_pusht_real">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_columbia_cairlab_pusht_real">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='columbia_cairlab_pusht_real',
    full_name='columbia_cairlab_pusht_real/0.1.0',
    description="""
    UR5 planar pushing tasks
    """,
    homepage='https://github.com/columbia-ai-robotics/diffusion_policy',
    data_path='gs://gresearch/robotics/columbia_cairlab_pusht_real/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=2.80 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(2,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=14, num_shards=4>,
        'train': <SplitInfo num_examples=122, num_shards=32>,
    },
    citation="""@inproceedings{chi2023diffusionpolicy,
    	title={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
    	author={Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
    	booktitle={Proceedings of Robotics: Science and Systems (RSS)},
    	year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="stanford_kuka_multimodal_dataset_converted_externally_to_rlds"> stanford_kuka_multimodal_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(3000 trajs, 31.98 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_stanford_kuka_multimodal_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_stanford_kuka_multimodal_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_stanford_kuka_multimodal_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_kuka_multimodal_dataset_converted_externally_to_rlds',
    full_name='stanford_kuka_multimodal_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Kuka iiwa peg insertion with force feedback
    """,
    homepage='https://sites.google.com/view/visionandtouch',
    data_path='gs://gresearch/robotics/stanford_kuka_multimodal_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=31.98 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'contact': Tensor(shape=(50,), dtype=float32),
                'depth_image': Tensor(shape=(128, 128, 1), dtype=float32),
                'ee_forces_continuous': Tensor(shape=(50, 6), dtype=float32),
                'ee_orientation': Tensor(shape=(4,), dtype=float32),
                'ee_orientation_vel': Tensor(shape=(3,), dtype=float32),
                'ee_position': Tensor(shape=(3,), dtype=float32),
                'ee_vel': Tensor(shape=(3,), dtype=float32),
                'ee_yaw': Tensor(shape=(4,), dtype=float32),
                'ee_yaw_delta': Tensor(shape=(4,), dtype=float32),
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
                'joint_vel': Tensor(shape=(7,), dtype=float32),
                'optical_flow': Tensor(shape=(128, 128, 2), dtype=float32),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=3000, num_shards=256>,
    },
    citation="""@inproceedings{lee2019icra,
      title={Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
      author={Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and  Garg, Animesh and Bohg, Jeannette},
      booktitle={2019 IEEE International Conference on Robotics and Automation (ICRA)},
      year={2019},
      url={https://arxiv.org/abs/1810.10191}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="nyu_rot_dataset_converted_externally_to_rlds"> nyu_rot_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(14 trajs, 5.33 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (84, 84, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_nyu_rot_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_nyu_rot_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_nyu_rot_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_rot_dataset_converted_externally_to_rlds',
    full_name='nyu_rot_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm short-horizon table-top tasks
    """,
    homepage='https://rot-robot.github.io/',
    data_path='gs://gresearch/robotics/nyu_rot_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=5.33 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(84, 84, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=14, num_shards=1>,
    },
    citation="""@inproceedings{haldar2023watch,
      title={Watch and match: Supercharging imitation with regularized optimal transport},
      author={Haldar, Siddhant and Mathur, Vaibhav and Yarats, Denis and Pinto, Lerrel},
      booktitle={Conference on Robot Learning},
      pages={32--43},
      year={2023},
      organization={PMLR}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="stanford_hydra_dataset_converted_externally_to_rlds"> stanford_hydra_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(570 trajs, 72.48 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (240, 320, 3) </th><th> wrist_image (240, 320, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_stanford_hydra_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_stanford_hydra_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_stanford_hydra_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_hydra_dataset_converted_externally_to_rlds',
    full_name='stanford_hydra_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka solving long-horizon tasks
    """,
    homepage='https://sites.google.com/view/hydra-il-2023',
    data_path='gs://gresearch/robotics/stanford_hydra_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=72.48 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_dense': Scalar(shape=(), dtype=bool),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'state': Tensor(shape=(27,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=570, num_shards=350>,
    },
    citation="""@article{belkhale2023hydra,
     title={HYDRA: Hybrid Robot Actions for Imitation Learning},
     author={Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
     journal={arxiv},
     year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="austin_buds_dataset_converted_externally_to_rlds"> austin_buds_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(50 trajs, 1.49 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th><th> wrist_image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_austin_buds_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_austin_buds_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_austin_buds_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_buds_dataset_converted_externally_to_rlds',
    full_name='austin_buds_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka stylized kitchen tasks
    """,
    homepage='https://ut-austin-rpl.github.io/rpl-BUDS/',
    data_path='gs://gresearch/robotics/austin_buds_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.49 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(24,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=50, num_shards=16>,
    },
    citation="""@article{zhu2022bottom,
      title={Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation},
      author={Zhu, Yifeng and Stone, Peter and Zhu, Yuke},
      journal={IEEE Robotics and Automation Letters},
      volume={7},
      number={2},
      pages={4126--4133},
      year={2022},
      publisher={IEEE}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="nyu_franka_play_dataset_converted_externally_to_rlds"> nyu_franka_play_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(365 trajs, 5.18 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image_additional_view (128, 128, 3) </th><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode0_image_additional_view.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode1_image_additional_view.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode2_image_additional_view.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_nyu_franka_play_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_nyu_franka_play_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_nyu_franka_play_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_franka_play_dataset_converted_externally_to_rlds',
    full_name='nyu_franka_play_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka interacting with toy kitchens
    """,
    homepage='https://play-to-policy.github.io/',
    data_path='gs://gresearch/robotics/nyu_franka_play_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=5.18 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(15,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth': Tensor(shape=(128, 128, 1), dtype=int32),
                'depth_additional_view': Tensor(shape=(128, 128, 1), dtype=int32),
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'image_additional_view': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=365, num_shards=32>,
        'val': <SplitInfo num_examples=91, num_shards=16>,
    },
    citation="""@article{cui2022play,
      title   = {From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data},
      author  = {Cui, Zichen Jeff and Wang, Yibin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
      journal = {arXiv preprint arXiv:2210.10047},
      year    = {2022}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="maniskill_dataset_converted_externally_to_rlds"> maniskill_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(30213 trajs, 151.05 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (256, 256, 3) </th><th> image (256, 256, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_maniskill_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_maniskill_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_maniskill_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='maniskill_dataset_converted_externally_to_rlds',
    full_name='maniskill_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Simulated Franka performing various manipulation tasks
    """,
    homepage='https://github.com/haosulab/ManiSkill2',
    data_path='gs://gresearch/robotics/maniskill_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=151.05 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'episode_id': Text(shape=(), dtype=string),
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'base_pose': Tensor(shape=(7,), dtype=float32),
                'depth': Image(shape=(256, 256, 1), dtype=uint16),
                'image': Image(shape=(256, 256, 3), dtype=uint8),
                'main_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32),
                'main_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32),
                'main_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32),
                'state': Tensor(shape=(18,), dtype=float32),
                'target_object_or_part_final_pose': Tensor(shape=(7,), dtype=float32),
                'target_object_or_part_final_pose_valid': Tensor(shape=(7,), dtype=uint8),
                'target_object_or_part_initial_pose': Tensor(shape=(7,), dtype=float32),
                'target_object_or_part_initial_pose_valid': Tensor(shape=(7,), dtype=uint8),
                'tcp_pose': Tensor(shape=(7,), dtype=float32),
                'wrist_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32),
                'wrist_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32),
                'wrist_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32),
                'wrist_depth': Image(shape=(256, 256, 1), dtype=uint16),
                'wrist_image': Image(shape=(256, 256, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=30213, num_shards=1024>,
    },
    citation="""@inproceedings{gu2023maniskill2,
      title={ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills},
      author={Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and Yuan, Xiaodi and Xie, Pengwei and Huang, Zhiao and Chen, Rui and Su, Hao},
      booktitle={International Conference on Learning Representations},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="cmu_franka_exploration_dataset_converted_externally_to_rlds"> cmu_franka_exploration_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(199 trajs, 602.24 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> highres_image (480, 640, 3) </th><th> image (64, 64, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode0_highres_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode1_highres_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode2_highres_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_cmu_franka_exploration_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_cmu_franka_exploration_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_cmu_franka_exploration_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_franka_exploration_dataset_converted_externally_to_rlds',
    full_name='cmu_franka_exploration_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka exploring toy kitchens
    """,
    homepage='https://human-world-model.github.io/',
    data_path='gs://gresearch/robotics/cmu_franka_exploration_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=602.24 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'highres_image': Image(shape=(480, 640, 3), dtype=uint8),
                'image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
            'structured_action': Tensor(shape=(8,), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=199, num_shards=8>,
    },
    citation="""@inproceedings{mendonca2023structured,
                  title={Structured World Models from Human Videos},
                  author={Mendonca, Russell  and Bahl, Shikhar and Pathak, Deepak},
                  journal={RSS},
                  year={2023}
                }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="ucsd_kitchen_dataset_converted_externally_to_rlds"> ucsd_kitchen_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(150 trajs, 1.33 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_ucsd_kitchen_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_ucsd_kitchen_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_ucsd_kitchen_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='ucsd_kitchen_dataset_converted_externally_to_rlds',
    full_name='ucsd_kitchen_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm interacting with different toy kitchens
    """,
    homepage=' ',
    data_path='gs://gresearch/robotics/ucsd_kitchen_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.33 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=150, num_shards=16>,
    },
    citation="""@ARTICLE{ucsd_kitchens,
      author = {Ge Yan, Kris Wu, and Xiaolong Wang},
      title = {{ucsd kitchens Dataset}},
      year = {2023},
      month = {August}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="ucsd_pick_and_place_dataset_converted_externally_to_rlds"> ucsd_pick_and_place_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(1355 trajs, 3.53 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (224, 224, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_ucsd_pick_and_place_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_ucsd_pick_and_place_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_ucsd_pick_and_place_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='ucsd_pick_and_place_dataset_converted_externally_to_rlds',
    full_name='ucsd_pick_and_place_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm picking and placing objects with distractors
    """,
    homepage='https://owmcorl.github.io',
    data_path='gs://gresearch/robotics/ucsd_pick_and_place_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=3.53 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'disclaimer': Text(shape=(), dtype=string),
            'file_path': Text(shape=(), dtype=string),
            'n_transitions': Scalar(shape=(), dtype=int32),
            'success': Scalar(shape=(), dtype=bool),
            'success_labeled_by': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=1355, num_shards=32>,
    },
    citation="""@preprint{Feng2023Finetuning,
    	title={Finetuning Offline World Models in the Real World},
    	author={Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang},
    	year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="austin_sailor_dataset_converted_externally_to_rlds"> austin_sailor_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(240 trajs, 18.85 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (128, 128, 3) </th><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_austin_sailor_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_austin_sailor_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_austin_sailor_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_sailor_dataset_converted_externally_to_rlds',
    full_name='austin_sailor_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka tablesetting tasks
    """,
    homepage='https://ut-austin-rpl.github.io/sailor/',
    data_path='gs://gresearch/robotics/austin_sailor_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=18.85 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
                'state_ee': Tensor(shape=(16,), dtype=float32),
                'state_gripper': Tensor(shape=(1,), dtype=float32),
                'state_joint': Tensor(shape=(7,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=240, num_shards=109>,
    },
    citation="""@inproceedings{nasiriany2022sailor,
          title={Learning and Retrieval from Prior Data for Skill-based Imitation Learning},
          author={Soroush Nasiriany and Tian Gao and Ajay Mandlekar and Yuke Zhu},
          booktitle={Conference on Robot Learning (CoRL)},
          year={2022}
        }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="austin_sirius_dataset_converted_externally_to_rlds"> austin_sirius_dataset_converted_externally_to_rlds 
    <small class="text-body-secondary">(559 trajs, 6.55 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (84, 84, 3) </th><th> image (84, 84, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_austin_sirius_dataset_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_austin_sirius_dataset_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_austin_sirius_dataset_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_sirius_dataset_converted_externally_to_rlds',
    full_name='austin_sirius_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka tabletop manipulation tasks
    """,
    homepage='https://ut-austin-rpl.github.io/sirius/',
    data_path='gs://gresearch/robotics/austin_sirius_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=6.55 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'action_mode': Tensor(shape=(1,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'intv_label': Tensor(shape=(1,), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(84, 84, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
                'state_ee': Tensor(shape=(16,), dtype=float32),
                'state_gripper': Tensor(shape=(1,), dtype=float32),
                'state_joint': Tensor(shape=(7,), dtype=float32),
                'wrist_image': Image(shape=(84, 84, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=559, num_shards=64>,
    },
    citation="""@inproceedings{liu2022robot,
        title = {Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment},
        author = {Huihan Liu and Soroush Nasiriany and Lance Zhang and Zhiyao Bao and Yuke Zhu},
        booktitle = {Robotics: Science and Systems (RSS)},
        year = {2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="bc_z"> bc_z 
    <small class="text-body-secondary">(39350 trajs, 80.54 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> natural_language_instruction </th><th> image (171, 213, 3) </th></tr>
<tr><td> place the white sponge in the ceramic bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bc_z/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the eraser on the white sponge </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bc_z/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> move the arm in a circular motion </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="bc_z/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_bc_z" role="button" aria-expanded="false" aria-controls="collapse_bc_z">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_bc_z">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='bc_z',
    full_name='bc_z/0.1.0',
    description="""
    Teleoped Google robot doing mostly pick-place from a table
    """,
    homepage='https://www.kaggle.com/datasets/google/bc-z-robot/discussion/309201',
    data_path='gs://gresearch/robotics/bc_z/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=80.54 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'future/axis_angle_residual': Tensor(shape=(30,), dtype=float32),
                'future/target_close': Tensor(shape=(10,), dtype=int64),
                'future/xyz_residual': Tensor(shape=(30,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'episode_success': float32,
                'image': Image(shape=(171, 213, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'present/autonomous': int64,
                'present/axis_angle': Tensor(shape=(3,), dtype=float32),
                'present/intervention': int64,
                'present/sensed_close': Tensor(shape=(1,), dtype=float32),
                'present/xyz': Tensor(shape=(3,), dtype=float32),
                'sequence_length': int64,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=39350, num_shards=1024>,
        'val': <SplitInfo num_examples=3914, num_shards=64>,
    },
    citation="""@inproceedings{jang2021bc,
    title={{BC}-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},
    author={Eric Jang and Alex Irpan and Mohi Khansari and Daniel Kappler and Frederik Ebert and Corey Lynch and Sergey Levine and Chelsea Finn},
    booktitle={5th Annual Conference on Robot Learning},
    year={2021},
    url={https://openreview.net/forum?id=8kbp23tSGYv}}""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="usc_cloth_sim_converted_externally_to_rlds"> usc_cloth_sim_converted_externally_to_rlds 
    <small class="text-body-secondary">(800 trajs, 254.52 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (32, 32, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_usc_cloth_sim_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_usc_cloth_sim_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_usc_cloth_sim_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='usc_cloth_sim_converted_externally_to_rlds',
    full_name='usc_cloth_sim_converted_externally_to_rlds/0.1.0',
    description="""
    Franka cloth interaction tasks
    """,
    homepage='https://uscresl.github.io/dmfd/',
    data_path='gs://gresearch/robotics/usc_cloth_sim_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=254.52 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(32, 32, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=800, num_shards=2>,
        'val': <SplitInfo num_examples=200, num_shards=1>,
    },
    citation="""@article{salhotra2022dmfd,
        author={Salhotra, Gautam and Liu, I-Chun Arthur and Dominguez-Kuhne, Marcus and Sukhatme, Gaurav S.},
        journal={IEEE Robotics and Automation Letters},
        title={Learning Deformable Object Manipulation From Expert Demonstrations},
        year={2022},
        volume={7},
        number={4},
        pages={8775-8782},
        doi={10.1109/LRA.2022.3187843}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utokyo_pr2_opening_fridge_converted_externally_to_rlds"> utokyo_pr2_opening_fridge_converted_externally_to_rlds 
    <small class="text-body-secondary">(64 trajs, 360.57 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utokyo_pr2_opening_fridge_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_utokyo_pr2_opening_fridge_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utokyo_pr2_opening_fridge_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_pr2_opening_fridge_converted_externally_to_rlds',
    full_name='utokyo_pr2_opening_fridge_converted_externally_to_rlds/0.1.0',
    description="""
    PR2 opening fridge doors
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_pr2_opening_fridge_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=360.57 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=64, num_shards=4>,
        'val': <SplitInfo num_examples=16, num_shards=1>,
    },
    citation="""@misc{oh2023pr2utokyodatasets,
      author={Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},
      title={X-Embodiment U-Tokyo PR2 Datasets},
      year={2023},
      url={https://github.com/ojh6404/rlds_dataset_builder},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds"> utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds 
    <small class="text-body-secondary">(192 trajs, 829.37 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds',
    full_name='utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/0.1.0',
    description="""
    tbd
    """,
    homepage='tbd',
    data_path='gs://gresearch/robotics/utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=829.37 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=192, num_shards=8>,
        'val': <SplitInfo num_examples=48, num_shards=2>,
    },
    citation="""tbd""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utokyo_saytap_converted_externally_to_rlds"> utokyo_saytap_converted_externally_to_rlds 
    <small class="text-body-secondary">(20 trajs, 55.34 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (64, 64, 3) </th><th> image (64, 64, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utokyo_saytap_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_utokyo_saytap_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utokyo_saytap_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_saytap_converted_externally_to_rlds',
    full_name='utokyo_saytap_converted_externally_to_rlds/0.1.0',
    description="""
    A1 walking, no RGB
    """,
    homepage='https://saytap.github.io/',
    data_path='gs://gresearch/robotics/utokyo_saytap_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=55.34 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(12,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'desired_pattern': Tensor(shape=(4, 5), dtype=bool),
                'desired_vel': Tensor(shape=(3,), dtype=float32),
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'prev_act': Tensor(shape=(12,), dtype=float32),
                'proj_grav_vec': Tensor(shape=(3,), dtype=float32),
                'state': Tensor(shape=(30,), dtype=float32),
                'wrist_image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=20, num_shards=1>,
    },
    citation="""@article{saytap2023,
      author = {Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and
    Tatsuya Harada},
      title  = {SayTap: Language to Quadrupedal Locomotion},
      eprint = {arXiv:2306.07580},
      url    = {https://saytap.github.io},
      note   = "{https://saytap.github.io}",
      year   = {2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utokyo_xarm_pick_and_place_converted_externally_to_rlds"> utokyo_xarm_pick_and_place_converted_externally_to_rlds 
    <small class="text-body-secondary">(92 trajs, 1.29 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (224, 224, 3) </th><th> hand_image (224, 224, 3) </th><th> image2 (224, 224, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utokyo_xarm_pick_and_place_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_utokyo_xarm_pick_and_place_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utokyo_xarm_pick_and_place_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_xarm_pick_and_place_converted_externally_to_rlds',
    full_name='utokyo_xarm_pick_and_place_converted_externally_to_rlds/0.1.0',
    description="""
    xArm picking and placing objects
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_xarm_pick_and_place_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.29 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_pose': Tensor(shape=(6,), dtype=float32),
                'hand_image': Image(shape=(224, 224, 3), dtype=uint8),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'image2': Image(shape=(224, 224, 3), dtype=uint8),
                'joint_state': Tensor(shape=(14,), dtype=float32),
                'joint_trajectory': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=92, num_shards=16>,
        'val': <SplitInfo num_examples=10, num_shards=1>,
    },
    citation="""@misc{matsushima2023weblab,
      title={Weblab xArm Dataset},
      author={Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utokyo_xarm_bimanual_converted_externally_to_rlds"> utokyo_xarm_bimanual_converted_externally_to_rlds 
    <small class="text-body-secondary">(64 trajs, 138.44 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (256, 256, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utokyo_xarm_bimanual_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_utokyo_xarm_bimanual_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utokyo_xarm_bimanual_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_xarm_bimanual_converted_externally_to_rlds',
    full_name='utokyo_xarm_bimanual_converted_externally_to_rlds/0.1.0',
    description="""
    xArm bimanual setup folding towel
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_xarm_bimanual_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=138.44 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(14,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'action_l': Tensor(shape=(7,), dtype=float32),
                'action_r': Tensor(shape=(7,), dtype=float32),
                'image': Image(shape=(256, 256, 3), dtype=uint8),
                'pose_l': Tensor(shape=(6,), dtype=float32),
                'pose_r': Tensor(shape=(6,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=64, num_shards=1>,
        'val': <SplitInfo num_examples=6, num_shards=1>,
    },
    citation="""@misc{matsushima2023weblab,
      title={Weblab xArm Dataset},
      author={Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="robo_net"> robo_net 
    <small class="text-body-secondary">(82775 trajs, 799.91 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image2 (240, 320, 3) </th><th> image (240, 320, 3) </th><th> image1 (240, 320, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode0_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode0_image1.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode1_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode1_image1.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode2_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="robo_net/episode2_image1.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_robo_net" role="button" aria-expanded="false" aria-controls="collapse_robo_net">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_robo_net">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='robo_net',
    full_name='robo_net/1.0.0',
    description="""
    This is an excerpt of the [RoboNet](https://github.com/SudeepDasari/RoboNet) dataset.
    
    
    Data from 5 robots randomly interacting with a bin using the AutoGrasp primitive. The action/state space is shared across all robots, and camera observations were taken using 3 synced camera images.
    """,
    homepage='https://www.tensorflow.org/datasets/catalog/robo_net',
    data_path='gs://gresearch/robotics/robo_net/1.0.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=799.91 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
            'robot': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(5,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': Scalar(shape=(), dtype=bool),
            'is_last': Scalar(shape=(), dtype=bool),
            'is_terminal': Scalar(shape=(), dtype=bool),
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'image1': Image(shape=(240, 320, 3), dtype=uint8),
                'image2': Image(shape=(240, 320, 3), dtype=uint8),
                'state': Tensor(shape=(5,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=82775, num_shards=1024>,
    },
    citation="""@inproceedings{dasari2019robonet,
        title={RoboNet: Large-Scale Multi-Robot Learning},
        author={Sudeep Dasari and Frederik Ebert and Stephen Tian and Suraj Nair and Bernadette Bucher and Karl Schmeckpeper and Siddharth Singh and Sergey Levine and Chelsea Finn},
        year={2019},
        eprint={1910.11215},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        booktitle={CoRL 2019: Volume 100 Proceedings of Machine Learning Research}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_mvp_converted_externally_to_rlds"> berkeley_mvp_converted_externally_to_rlds 
    <small class="text-body-secondary">(480 trajs, 12.34 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> hand_image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_mvp_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_berkeley_mvp_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_mvp_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_mvp_converted_externally_to_rlds',
    full_name='berkeley_mvp_converted_externally_to_rlds/0.1.0',
    description="""
    xArm performing 6 manipulation tasks
    """,
    homepage='https://arxiv.org/abs/2203.06173',
    data_path='gs://gresearch/robotics/berkeley_mvp_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=12.34 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'gripper': Scalar(shape=(), dtype=bool),
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
                'pose': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=480, num_shards=124>,
    },
    citation="""@InProceedings{Radosavovic2022,
      title = {Real-World Robot Learning with Masked Visual Pre-training},
      author = {Ilija Radosavovic and Tete Xiao and Stephen James and Pieter Abbeel and Jitendra Malik and Trevor Darrell},
      booktitle = {CoRL},
      year = {2022}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_rpt_converted_externally_to_rlds"> berkeley_rpt_converted_externally_to_rlds 
    <small class="text-body-secondary">(908 trajs, 40.64 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> hand_image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_rpt_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_berkeley_rpt_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_rpt_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_rpt_converted_externally_to_rlds',
    full_name='berkeley_rpt_converted_externally_to_rlds/0.1.0',
    description="""
    Franka performing tabletop pick place tasks
    """,
    homepage='https://arxiv.org/abs/2306.10007',
    data_path='gs://gresearch/robotics/berkeley_rpt_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=40.64 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'gripper': Scalar(shape=(), dtype=bool),
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=908, num_shards=441>,
    },
    citation="""@article{Radosavovic2023,
      title={Robot Learning with Sensorimotor Pre-training},
      author={Ilija Radosavovic and Baifeng Shi and Letian Fu and Ken Goldberg and Trevor Darrell and Jitendra Malik},
      year={2023},
      journal={arXiv:2306.10007}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="kaist_nonprehensile_converted_externally_to_rlds"> kaist_nonprehensile_converted_externally_to_rlds 
    <small class="text-body-secondary">(201 trajs, 11.71 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_kaist_nonprehensile_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_kaist_nonprehensile_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_kaist_nonprehensile_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='kaist_nonprehensile_converted_externally_to_rlds',
    full_name='kaist_nonprehensile_converted_externally_to_rlds/0.1.0',
    description="""
    Franka manipulating ungraspable objects
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/kaist_nonprehensile_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=11.71 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(20,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'partial_pointcloud': Tensor(shape=(512, 3), dtype=float32),
                'state': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=201, num_shards=101>,
    },
    citation="""@article{kimpre,
      title={Pre-and post-contact policy decomposition for non-prehensile manipulation with zero-shot sim-to-real transfer},
      author={Kim, Minchan and Han, Junhyek and Kim, Jaehyung and Kim, Beomjoon},
      booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="stanford_mask_vit_converted_externally_to_rlds"> stanford_mask_vit_converted_externally_to_rlds 
    <small class="text-body-secondary">(9109 trajs, 76.17 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (480, 480, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_stanford_mask_vit_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_stanford_mask_vit_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_stanford_mask_vit_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_mask_vit_converted_externally_to_rlds',
    full_name='stanford_mask_vit_converted_externally_to_rlds/0.1.0',
    description="""
    Sawyer pushing and picking objects in a bin
    """,
    homepage='https://arxiv.org/abs/2206.11894',
    data_path='gs://gresearch/robotics/stanford_mask_vit_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=76.17 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(5,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_pose': Tensor(shape=(5,), dtype=float32),
                'finger_sensors': Tensor(shape=(1,), dtype=float32),
                'high_bound': Tensor(shape=(5,), dtype=float32),
                'image': Image(shape=(480, 480, 3), dtype=uint8),
                'low_bound': Tensor(shape=(5,), dtype=float32),
                'state': Tensor(shape=(15,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=9109, num_shards=1023>,
        'val': <SplitInfo num_examples=91, num_shards=8>,
    },
    citation="""@inproceedings{gupta2022maskvit,
      title={MaskViT: Masked Visual Pre-Training for Video Prediction},
      author={Agrim Gupta and Stephen Tian and Yunzhi Zhang and Jiajun Wu and Roberto Martn-Martn and Li Fei-Fei},
      booktitle={International Conference on Learning Representations},
      year={2022}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="tokyo_u_lsmo_converted_externally_to_rlds"> tokyo_u_lsmo_converted_externally_to_rlds 
    <small class="text-body-secondary">(50 trajs, 335.71 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (120, 120, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_tokyo_u_lsmo_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_tokyo_u_lsmo_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_tokyo_u_lsmo_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='tokyo_u_lsmo_converted_externally_to_rlds',
    full_name='tokyo_u_lsmo_converted_externally_to_rlds/0.1.0',
    description="""
    motion planning trajectory of pick place tasks
    """,
    homepage='https://journals.sagepub.com/doi/full/10.1177/02783649211044405',
    data_path='gs://gresearch/robotics/tokyo_u_lsmo_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=335.71 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 120, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=50, num_shards=4>,
    },
    citation="""@Article{Osa22,
      author  = {Takayuki Osa},
      journal = {The International Journal of Robotics Research},
      title   = {Motion Planning by Learning the Solution Manifold in Trajectory Optimization},
      year    = {2022},
      number  = {3},
      pages   = {291--311},
      volume  = {41},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="dlr_sara_pour_converted_externally_to_rlds"> dlr_sara_pour_converted_externally_to_rlds 
    <small class="text-body-secondary">(100 trajs, 2.92 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_dlr_sara_pour_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_dlr_sara_pour_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_dlr_sara_pour_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_sara_pour_converted_externally_to_rlds',
    full_name='dlr_sara_pour_converted_externally_to_rlds/0.1.0',
    description="""
    pouring liquid from a bottle into a mug
    """,
    homepage='https://elib.dlr.de/193739/1/padalkar2023rlsct.pdf',
    data_path='gs://gresearch/robotics/dlr_sara_pour_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=2.92 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(6,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=100, num_shards=31>,
    },
    citation="""@inproceedings{padalkar2023guiding,
      title={Guiding Reinforcement Learning with Shared Control Templates},
      author={Padalkar, Abhishek and Quere, Gabriel and Steinmetz, Franz and Raffin, Antonin and Nieuwenhuisen, Matthias and Silv{'e}rio, Jo{\~a}o and Stulp, Freek},
      booktitle={40th IEEE International Conference on Robotics and Automation, ICRA 2023},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="dlr_sara_grid_clamp_converted_externally_to_rlds"> dlr_sara_grid_clamp_converted_externally_to_rlds 
    <small class="text-body-secondary">(107 trajs, 1.65 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (480, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_dlr_sara_grid_clamp_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_dlr_sara_grid_clamp_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_dlr_sara_grid_clamp_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_sara_grid_clamp_converted_externally_to_rlds',
    full_name='dlr_sara_grid_clamp_converted_externally_to_rlds/0.1.0',
    description="""
    place grid clamp onto grids on table
    """,
    homepage='https://www.researchsquare.com/article/rs-3289569/v1',
    data_path='gs://gresearch/robotics/dlr_sara_grid_clamp_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.65 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(12,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=107, num_shards=16>,
    },
    citation="""@article{padalkar2023guided,
      title={A guided reinforcement learning approach using shared control templates for learning manipulation skills in the real world},
      author={Padalkar, Abhishek and Quere, Gabriel and Raffin, Antonin and Silv{'e}rio, Jo{\~a}o and Stulp, Freek},
      journal={Research square preprint rs-3289569/v1},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="dlr_edan_shared_control_converted_externally_to_rlds"> dlr_edan_shared_control_converted_externally_to_rlds 
    <small class="text-body-secondary">(104 trajs, 3.09 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (360, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_dlr_edan_shared_control_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_dlr_edan_shared_control_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_dlr_edan_shared_control_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_edan_shared_control_converted_externally_to_rlds',
    full_name='dlr_edan_shared_control_converted_externally_to_rlds/0.1.0',
    description="""
    wheelchair with arm performing shelf pick tasks
    """,
    homepage='https://ieeexplore.ieee.org/document/9341156',
    data_path='gs://gresearch/robotics/dlr_edan_shared_control_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=3.09 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=104, num_shards=29>,
    },
    citation="""@inproceedings{vogel_edan_2020,
    	title = {EDAN - an EMG-Controlled Daily Assistant to Help People with Physical Disabilities},
    	language = {en},
    	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
    	author = {Vogel, Jrn and Hagengruber, Annette and Iskandar, Maged and Quere, Gabriel and Leipscher, Ulrike and Bustamante, Samuel and Dietrich, Alexander and Hoeppner, Hannes and Leidner, Daniel and Albu-Schffer, Alin},
    	year = {2020}
    }
    @inproceedings{quere_shared_2020,
    	address = {Paris, France},
    	title = {Shared {Control} {Templates} for {Assistive} {Robotics}},
    	language = {en},
    	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
    	author = {Quere, Gabriel and Hagengruber, Annette and Iskandar, Maged and Bustamante, Samuel and Leidner, Daniel and Stulp, Freek and Vogel, Joern},
    	year = {2020},
    	pages = {7},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="asu_table_top_converted_externally_to_rlds"> asu_table_top_converted_externally_to_rlds 
    <small class="text-body-secondary">(110 trajs, 737.60 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (224, 224, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="asu_table_top_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="asu_table_top_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="asu_table_top_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_asu_table_top_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_asu_table_top_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_asu_table_top_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='asu_table_top_converted_externally_to_rlds',
    full_name='asu_table_top_converted_externally_to_rlds/0.1.0',
    description="""
    UR5 performing table-top pick/place/rotate tasks
    """,
    homepage='https://link.springer.com/article/10.1007/s10514-023-10129-1',
    data_path='gs://gresearch/robotics/asu_table_top_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=737.60 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'action_delta': Tensor(shape=(7,), dtype=float32),
            'action_inst': Text(shape=(), dtype=string),
            'discount': Scalar(shape=(), dtype=float32),
            'goal_object': Text(shape=(), dtype=string),
            'ground_truth_states': FeaturesDict({
                'EE': Tensor(shape=(6,), dtype=float32),
                'bottle': Tensor(shape=(6,), dtype=float32),
                'bread': Tensor(shape=(6,), dtype=float32),
                'coke': Tensor(shape=(6,), dtype=float32),
                'cube': Tensor(shape=(6,), dtype=float32),
                'milk': Tensor(shape=(6,), dtype=float32),
                'pepsi': Tensor(shape=(6,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
                'state_vel': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=110, num_shards=8>,
    },
    citation="""@inproceedings{zhou2023modularity,
      title={Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation},
      author={Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Stepputtis, Simon and Amor, Heni},
      booktitle={Conference on Robot Learning},
      pages={1684--1695},
      year={2023},
      organization={PMLR}
    }
    @article{zhou2023learning,
      title={Learning modular language-conditioned robot policies through attention},
      author={Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Ben Amor, Heni and Stepputtis, Simon},
      journal={Autonomous Robots},
      pages={1--21},
      year={2023},
      publisher={Springer}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="stanford_robocook_converted_externally_to_rlds"> stanford_robocook_converted_externally_to_rlds 
    <small class="text-body-secondary">(2460 trajs, 124.62 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image_3 (256, 256, 3) </th><th> image_2 (256, 256, 3) </th><th> image_4 (256, 256, 3) </th><th> image_1 (256, 256, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_1.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_1.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_1.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_stanford_robocook_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_stanford_robocook_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_stanford_robocook_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_robocook_converted_externally_to_rlds',
    full_name='stanford_robocook_converted_externally_to_rlds/0.1.0',
    description="""
    Franka preparing dumplings with various tools
    """,
    homepage='https://hshi74.github.io/robocook/',
    data_path='gs://gresearch/robotics/stanford_robocook_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=124.62 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'extrinsics_1': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_2': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_3': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_4': Tensor(shape=(4, 4), dtype=float32),
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth_1': Tensor(shape=(256, 256), dtype=float32),
                'depth_2': Tensor(shape=(256, 256), dtype=float32),
                'depth_3': Tensor(shape=(256, 256), dtype=float32),
                'depth_4': Tensor(shape=(256, 256), dtype=float32),
                'image_1': Image(shape=(256, 256, 3), dtype=uint8),
                'image_2': Image(shape=(256, 256, 3), dtype=uint8),
                'image_3': Image(shape=(256, 256, 3), dtype=uint8),
                'image_4': Image(shape=(256, 256, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=2460, num_shards=923>,
    },
    citation="""@article{shi2023robocook,
      title={RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools},
      author={Shi, Haochen and Xu, Huazhe and Clarke, Samuel and Li, Yunzhu and Wu, Jiajun},
      journal={arXiv preprint arXiv:2306.14447},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="eth_agent_affordances"> eth_agent_affordances 
    <small class="text-body-secondary">(118 trajs, 17.27 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (64, 64, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="eth_agent_affordances/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="eth_agent_affordances/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="eth_agent_affordances/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_eth_agent_affordances" role="button" aria-expanded="false" aria-controls="collapse_eth_agent_affordances">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_eth_agent_affordances">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='eth_agent_affordances',
    full_name='eth_agent_affordances/0.1.0',
    description="""
    Franka opening ovens -- point cloud + proprio only
    """,
    homepage='https://ieeexplore.ieee.org/iel7/10160211/10160212/10160747.pdf',
    data_path='gs://gresearch/robotics/eth_agent_affordances/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=17.27 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
            'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(6,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=118, num_shards=53>,
    },
    citation="""@inproceedings{schiavi2023learning,
      title={Learning agent-aware affordances for closed-loop interaction with articulated objects},
      author={Schiavi, Giulio and Wulkop, Paula and Rizzi, Giuseppe and Ott, Lionel and Siegwart, Roland and Chung, Jen Jen},
      booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
      pages={5916--5922},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="imperialcollege_sawyer_wrist_cam"> imperialcollege_sawyer_wrist_cam 
    <small class="text-body-secondary">(170 trajs, 81.87 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (64, 64, 3) </th><th> image (64, 64, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="imperialcollege_sawyer_wrist_cam/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_imperialcollege_sawyer_wrist_cam" role="button" aria-expanded="false" aria-controls="collapse_imperialcollege_sawyer_wrist_cam">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_imperialcollege_sawyer_wrist_cam">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='imperialcollege_sawyer_wrist_cam',
    full_name='imperialcollege_sawyer_wrist_cam/0.1.0',
    description="""
    Sawyer performing table top manipulation
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/imperialcollege_sawyer_wrist_cam/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=81.87 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'state': Tensor(shape=(1,), dtype=float32),
                'wrist_image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=170, num_shards=1>,
    },
    citation="""--""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="iamlab_cmu_pickup_insert_converted_externally_to_rlds"> iamlab_cmu_pickup_insert_converted_externally_to_rlds 
    <small class="text-body-secondary">(631 trajs, 50.29 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (360, 640, 3) </th><th> wrist_image (240, 320, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_iamlab_cmu_pickup_insert_converted_externally_to_rlds" role="button" aria-expanded="false" aria-controls="collapse_iamlab_cmu_pickup_insert_converted_externally_to_rlds">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_iamlab_cmu_pickup_insert_converted_externally_to_rlds">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='iamlab_cmu_pickup_insert_converted_externally_to_rlds',
    full_name='iamlab_cmu_pickup_insert_converted_externally_to_rlds/0.1.0',
    description="""
    Franka picking objects and insertion tasks
    """,
    homepage='https://openreview.net/forum?id=WuBv9-IGDUA',
    data_path='gs://gresearch/robotics/iamlab_cmu_pickup_insert_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=50.29 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(20,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=631, num_shards=369>,
    },
    citation="""@inproceedings{
    saxena2023multiresolution,
    title={Multi-Resolution Sensing for Real-Time Control with Vision-Language Models},
    author={Saumya Saxena and Mohit Sharma and Oliver Kroemer},
    booktitle={7th Annual Conference on Robot Learning},
    year={2023},
    url={https://openreview.net/forum?id=WuBv9-IGDUA}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="uiuc_d3field"> uiuc_d3field 
    <small class="text-body-secondary">(192 trajs, 15.82 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image_1 (360, 640, 3) </th><th> image_3 (360, 640, 3) </th><th> image_2 (360, 640, 3) </th><th> image_4 (360, 640, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode0_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode0_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode0_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode0_image_4.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode1_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode1_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode1_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode1_image_4.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode2_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode2_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode2_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="uiuc_d3field/episode2_image_4.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_uiuc_d3field" role="button" aria-expanded="false" aria-controls="collapse_uiuc_d3field">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_uiuc_d3field">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='uiuc_d3field',
    full_name='uiuc_d3field/0.1.0',
    description="""
    Organizing office desk, utensils etc
    """,
    homepage='https://robopil.github.io/d3fields/',
    data_path='gs://gresearch/robotics/uiuc_d3field/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=15.82 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(3,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth_1': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_2': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_3': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_4': Image(shape=(360, 640, 1), dtype=uint16),
                'image_1': Image(shape=(360, 640, 3), dtype=uint8),
                'image_2': Image(shape=(360, 640, 3), dtype=uint8),
                'image_3': Image(shape=(360, 640, 3), dtype=uint8),
                'image_4': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(4, 4), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=192, num_shards=98>,
    },
    citation="""@article{wang2023d3field,
      title={D^3Field: Dynamic 3D Descriptor Fields for Generalizable Robotic Manipulation}, 
      author={Wang, Yixuan and Li, Zhuoran and Zhang, Mingtong and Driggs-Campbell, Katherine and Wu, Jiajun and Fei-Fei, Li and Li, Yunzhu},
      journal={arXiv preprint arXiv:},
      year={2023},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="utaustin_mutex"> utaustin_mutex 
    <small class="text-body-secondary">(1500 trajs, 20.79 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th><th> wrist_image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="utaustin_mutex/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_utaustin_mutex" role="button" aria-expanded="false" aria-controls="collapse_utaustin_mutex">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_utaustin_mutex">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utaustin_mutex',
    full_name='utaustin_mutex/0.1.0',
    description="""
    Diverse household manipulation tasks
    """,
    homepage='https://ut-austin-rpl.github.io/MUTEX/',
    data_path='gs://gresearch/robotics/utaustin_mutex/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=20.79 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(24,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=1500, num_shards=256>,
    },
    citation="""@inproceedings{
        shah2023mutex,
        title={{MUTEX}: Learning Unified Policies from Multimodal Task Specifications},
        author={Rutav Shah and Roberto Mart{'\i}n-Mart{'\i}n and Yuke Zhu},
        booktitle={7th Annual Conference on Robot Learning},
        year={2023},
        url={https://openreview.net/forum?id=PwqiqaaEzJ}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_fanuc_manipulation"> berkeley_fanuc_manipulation 
    <small class="text-body-secondary">(415 trajs, 8.85 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> wrist_image (224, 224, 3) </th><th> image (224, 224, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_fanuc_manipulation/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_fanuc_manipulation" role="button" aria-expanded="false" aria-controls="collapse_berkeley_fanuc_manipulation">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_fanuc_manipulation">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_fanuc_manipulation',
    full_name='berkeley_fanuc_manipulation/0.1.0',
    description="""
    Fanuc robot performing various manipulation tasks
    """,
    homepage='https://sites.google.com/berkeley.edu/fanuc-manipulation',
    data_path='gs://gresearch/robotics/berkeley_fanuc_manipulation/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=8.85 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(6,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_state': Tensor(shape=(7,), dtype=float32),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
                'wrist_image': Image(shape=(224, 224, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=415, num_shards=124>,
    },
    citation="""@article{fanuc_manipulation2023,
      title={Fanuc Manipulation: A Dataset for Learning-based Manipulation with FANUC Mate 200iD Robot},
      author={Zhu, Xinghao and Tian, Ran and Xu, Chenfeng and Ding, Mingyu and Zhan, Wei and Tomizuka, Masayoshi},
      year={2023},
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="cmu_play_fusion"> cmu_play_fusion 
    <small class="text-body-secondary">(576 trajs, 6.68 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_play_fusion/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_play_fusion/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_play_fusion/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_cmu_play_fusion" role="button" aria-expanded="false" aria-controls="collapse_cmu_play_fusion">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_cmu_play_fusion">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_play_fusion',
    full_name='cmu_play_fusion/0.1.0',
    description="""
    The robot plays with 3 complex scenes: a grill with many cooking objects like toaster, pan, etc. It has to pick, open, place, close. It  has to set a table, move plates, cups, utensils. And it has to place dishes in the sink, dishwasher, hand cups etc.
    """,
    homepage='https://play-fusion.github.io/',
    data_path='gs://gresearch/robotics/cmu_play_fusion/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=6.68 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(9,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=576, num_shards=64>,
    },
    citation="""@inproceedings{chen2023playfusion,
      title={PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play},
      author={Chen, Lili and Bahl, Shikhar and Pathak, Deepak},
      booktitle={CoRL},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="cmu_stretch"> cmu_stretch 
    <small class="text-body-secondary">(135 trajs, 728.06 MiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (128, 128, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_stretch/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_stretch/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="cmu_stretch/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_cmu_stretch" role="button" aria-expanded="false" aria-controls="collapse_cmu_stretch">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_cmu_stretch">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_stretch',
    full_name='cmu_stretch/0.1.0',
    description="""
    Hello stretch robot kitchen interactions
    """,
    homepage='https://robo-affordances.github.io/',
    data_path='gs://gresearch/robotics/cmu_stretch/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=728.06 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(4,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=135, num_shards=8>,
    },
    citation="""@inproceedings{bahl2023affordances,
      title={Affordances from Human Videos as a Versatile Representation for Robotics},
      author={Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},
      booktitle={CVPR},
      year={2023}
    }
    @article{mendonca2023structured,
      title={Structured World Models from Human Videos},
      author={Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
      journal={CoRL},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_gnm_recon"> berkeley_gnm_recon 
    <small class="text-body-secondary">(11834 trajs, 18.73 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (120, 160, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_recon/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_recon/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_recon/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_gnm_recon" role="button" aria-expanded="false" aria-controls="collapse_berkeley_gnm_recon">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_gnm_recon">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_recon',
    full_name='berkeley_gnm_recon/0.1.0',
    description="""
    off-road navigation
    """,
    homepage='https://sites.google.com/view/recon-robot',
    data_path='gs://gresearch/robotics/berkeley_gnm_recon/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=18.73 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 160, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=11834, num_shards=256>,
    },
    citation="""@inproceedings{
    shah2021rapid,
    title={{Rapid Exploration for Open-World Navigation with Latent Goal Models}},
    author={Dhruv Shah and Benjamin Eysenbach and Nicholas Rhinehart and Sergey Levine},
    booktitle={5th Annual Conference on Robot Learning },
    year={2021},
    url={https://openreview.net/forum?id=d_SWJhyKfVw}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_gnm_cory_hall"> berkeley_gnm_cory_hall 
    <small class="text-body-secondary">(7331 trajs, 1.39 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (64, 85, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_cory_hall/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_cory_hall/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_cory_hall/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_gnm_cory_hall" role="button" aria-expanded="false" aria-controls="collapse_berkeley_gnm_cory_hall">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_gnm_cory_hall">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_cory_hall',
    full_name='berkeley_gnm_cory_hall/0.1.0',
    description="""
    hallway navigation
    """,
    homepage='https://arxiv.org/abs/1709.10489',
    data_path='gs://gresearch/robotics/berkeley_gnm_cory_hall/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.39 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 85, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=7331, num_shards=16>,
    },
    citation="""@inproceedings{kahn2018self,
      title={Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation},
      author={Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
      booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
      pages={5129--5136},
      year={2018},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    </div>
    
    
    <hr>
    <div class="dataset">
    <div class="row mt-5">
    <h2 id="berkeley_gnm_sac_son"> berkeley_gnm_sac_son 
    <small class="text-body-secondary">(2955 trajs, 7.00 GiB)</small></h2>
    </div>
    <table class="display_table table">
<tr><th> image (120, 160, 3) </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_sac_son/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_sac_son/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background" style="max-height: 320px">
      <source src="berkeley_gnm_sac_son/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <a data-bs-toggle="collapse" href="#collapse_berkeley_gnm_sac_son" role="button" aria-expanded="false" aria-controls="collapse_berkeley_gnm_sac_son">
    See Full Dataset Info
  </a>
  <div class="collapse" id="collapse_berkeley_gnm_sac_son">
    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_sac_son',
    full_name='berkeley_gnm_sac_son/0.1.0',
    description="""
    office navigation
    """,
    homepage='https://sites.google.com/view/SACSoN-review',
    data_path='gs://gresearch/robotics/berkeley_gnm_sac_son/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=7.00 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 160, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=2955, num_shards=64>,
    },
    citation="""@article{hirose2023sacson,
      title={SACSoN: Scalable Autonomous Data Collection for Social Navigation},
      author={Hirose, Noriaki and Shah, Dhruv and Sridhar, Ajay and Levine, Sergey},
      journal={arXiv preprint arXiv:2306.01874},
      year={2023}
    }""",
)
    </pre>
    </div>
    </div>
    
    
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
</div>
</body>
</html>
