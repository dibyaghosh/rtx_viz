

    <div class="dataset">
    <h2 id="fractal20220817_data"> fractal20220817_data </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> pick water bottle </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="fractal20220817_data/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick apple from white bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="fractal20220817_data/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> open bottom drawer </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="fractal20220817_data/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='fractal20220817_data',
    full_name='fractal20220817_data/0.1.0',
    description="""
    Table-top manipulation with 17 objects
    """,
    homepage='https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html',
    data_path='gs://gresearch/robotics/fractal20220817_data/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=111.07 GiB,
    features=FeaturesDict({
        'aspects': FeaturesDict({
            'already_success': bool,
            'feasible': bool,
            'has_aspects': bool,
            'success': bool,
            'undesirable': bool,
        }),
        'attributes': FeaturesDict({
            'collection_mode': int64,
            'collection_mode_name': string,
            'data_type': int64,
            'data_type_name': string,
            'env': int64,
            'env_name': string,
            'location': int64,
            'location_name': string,
            'objects_family': int64,
            'objects_family_name': string,
            'task_family': int64,
            'task_family_name': string,
        }),
        'steps': Dataset({
            'action': FeaturesDict({
                'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
                'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
                'gripper_closed': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_commanded': Tensor(shape=(1,), dtype=float32),
                'height_to_bottom': Tensor(shape=(1,), dtype=float32),
                'image': Image(shape=(256, 320, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'orientation_box': Tensor(shape=(2, 3), dtype=float32),
                'orientation_start': Tensor(shape=(4,), dtype=float32),
                'robot_orientation_positions_box': Tensor(shape=(3, 3), dtype=float32),
                'rotation_delta_to_go': Tensor(shape=(3,), dtype=float32),
                'src_rotation': Tensor(shape=(4,), dtype=float32),
                'vector_to_go': Tensor(shape=(3,), dtype=float32),
                'workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=87212, num_shards=1024>,
    },
    citation="""@article{brohan2022rt,
      title={Rt-1: Robotics transformer for real-world control at scale},
      author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
      journal={arXiv preprint arXiv:2212.06817},
      year={2022}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="kuka"> kuka </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kuka/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kuka/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pick anything </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kuka/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='kuka',
    full_name='kuka/0.1.0',
    description="""
    Bin picking and rearrangement tasks
    """,
    homepage='https://arxiv.org/abs/1806.10293',
    data_path='gs://gresearch/robotics/kuka/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=778.02 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
                'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'clip_function_input/base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
                'clip_function_input/workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
                'gripper_closed': Tensor(shape=(1,), dtype=float32),
                'height_to_bottom': Tensor(shape=(1,), dtype=float32),
                'image': Image(shape=(512, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'task_id': Tensor(shape=(1,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
        'success': bool,
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=580392, num_shards=1024>,
    },
    citation="""@article{kalashnikov2018qt,
      title={Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
      author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
      journal={arXiv preprint arXiv:1806.10293},
      year={2018}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="bridge"> bridge </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> PICK UP THE SPOON AND PUT NEAR THE VESSEL </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bridge/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the pot on the left side of the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bridge/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> Slide the cloth to the edge of the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bridge/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='bridge',
    full_name='bridge/0.1.0',
    description="""
    WidowX interacting with toy kitchens
    """,
    homepage='https://rail-berkeley.github.io/bridgedata/',
    data_path='gs://gresearch/robotics/bridge/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=387.49 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'open_gripper': bool,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=3475, num_shards=512>,
        'train': <SplitInfo num_examples=25460, num_shards=1024>,
    },
    citation="""@inproceedings{walke2023bridgedata,
        title={BridgeData V2: A Dataset for Robot Learning at Scale},
        author={Walke, Homer and Black, Kevin and Lee, Abraham and Kim, Moo Jin and Du, Max and Zheng, Chongyi and Zhao, Tony and Hansen-Estruch, Philippe and Vuong, Quan and He, Andre and Myers, Vivek and Fang, Kuan and Finn, Chelsea and Levine, Sergey},
        booktitle={Conference on Robot Learning (CoRL)},
        year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="taco_play"> taco_play </h2>
    <table class="display_table">
<tr><th> structured_language_instruction </th><th> natural_language_instruction </th><th> rgb_gripper </th><th> rgb_static </th></tr>
<tr><td> lift_pink_block </td><td> pick up the pink block </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode0_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode0_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> lift_pink_block </td><td> go towards the pink block and pick it up </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode1_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode1_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> lift_purple_block </td><td> grasp the purple block </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode2_rgb_gripper.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="taco_play/episode2_rgb_static.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='taco_play',
    full_name='taco_play/0.1.0',
    description="""
    Franka arm interacting with kitchen
    """,
    homepage='https://www.kaggle.com/datasets/oiermees/taco-robot',
    data_path='gs://gresearch/robotics/taco_play/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=47.77 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'actions': Tensor(shape=(7,), dtype=float32),
                'rel_actions_gripper': Tensor(shape=(7,), dtype=float32),
                'rel_actions_world': Tensor(shape=(7,), dtype=float32),
                'terminate_episode': float32,
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'depth_gripper': Tensor(shape=(84, 84), dtype=float32),
                'depth_static': Tensor(shape=(150, 200), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'rgb_gripper': Image(shape=(84, 84, 3), dtype=uint8),
                'rgb_static': Image(shape=(150, 200, 3), dtype=uint8),
                'robot_obs': Tensor(shape=(15,), dtype=float32),
                'structured_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=361, num_shards=64>,
        'train': <SplitInfo num_examples=3242, num_shards=511>,
    },
    citation="""@inproceedings{rosete2022tacorl,
    author = {Erick Rosete-Beas and Oier Mees and Gabriel Kalweit and Joschka Boedecker and Wolfram Burgard},
    title = {Latent Plans for Task Agnostic Offline Reinforcement Learning},
    journal = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
    year = {2022}
    }
    @inproceedings{mees23hulc2,
    title={Grounding  Language  with  Visual  Affordances  over  Unstructured  Data},
    author={Oier Mees and Jessica Borja-Diaz and Wolfram Burgard},
    booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    year={2023},
    address = {London, UK}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="jaco_play"> jaco_play </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th><th> image_wrist </th></tr>
<tr><td> place the butter dairy in the sink </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode0_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the gray bowl in the dish rack </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode1_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the long bread in the gray bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="jaco_play/episode2_image_wrist.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='jaco_play',
    full_name='jaco_play/0.1.0',
    description="""
    Jaco 2 pick place on table top
    """,
    homepage='https://github.com/clvrai/clvr_jaco_play_dataset',
    data_path='gs://gresearch/robotics/jaco_play/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=9.24 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'terminate_episode': Tensor(shape=(3,), dtype=int32),
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'end_effector_cartesian_pos': Tensor(shape=(7,), dtype=float32),
                'end_effector_cartesian_velocity': Tensor(shape=(6,), dtype=float32),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'image_wrist': Image(shape=(224, 224, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(8,), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=109, num_shards=8>,
        'train': <SplitInfo num_examples=976, num_shards=128>,
    },
    citation="""@software{dass2023jacoplay,
      author = {Dass, Shivin and Yapeter, Jullian and Zhang, Jesse and Zhang, Jiahui
                and Pertsch, Karl and Nikolaidis, Stefanos and Lim, Joseph J.},
      title = {CLVR Jaco Play Dataset},
      url = {https://github.com/clvrai/clvr_jaco_play_dataset},
      version = {1.0.0},
      year = {2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_cable_routing"> berkeley_cable_routing </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> top_image </th><th> wrist45_image </th><th> image </th><th> wrist225_image </th></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode0_top_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode0_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode0_wrist225_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode1_top_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode1_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode1_wrist225_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> route cable </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode2_top_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode2_wrist45_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_cable_routing/episode2_wrist225_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_cable_routing',
    full_name='berkeley_cable_routing/0.1.0',
    description="""
    Routing cable into clamps on table top
    """,
    homepage='https://sites.google.com/view/cablerouting/home',
    data_path='gs://gresearch/robotics/berkeley_cable_routing/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=4.67 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(7,), dtype=float32),
                'top_image': Image(shape=(128, 128, 3), dtype=uint8),
                'wrist225_image': Image(shape=(128, 128, 3), dtype=uint8),
                'wrist45_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=165, num_shards=4>,
        'train': <SplitInfo num_examples=1482, num_shards=64>,
    },
    citation="""@article{luo2023multistage,
      author    = {Jianlan Luo and Charles Xu and Xinyang Geng and Gilbert Feng and Kuan Fang and Liam Tan and Stefan Schaal and Sergey Levine},
      title     = {Multi-Stage Cable Routing through Hierarchical Imitation Learning},
      journal   = {arXiv pre-print},
      year      = {2023},
      url       = {https://arxiv.org/abs/2307.08927},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="roboturk"> roboturk </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> front_rgb </th></tr>
<tr><td> layout laundry </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="roboturk/episode0_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> layout laundry </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="roboturk/episode1_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> object search </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="roboturk/episode2_front_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='roboturk',
    full_name='roboturk/0.1.0',
    description="""
    Cloth folding, bowl stacking
    """,
    homepage='https://roboturk.stanford.edu/dataset_real.html',
    data_path='gs://gresearch/robotics/roboturk/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=45.39 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'front_rgb': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=199, num_shards=60>,
        'train': <SplitInfo num_examples=1796, num_shards=494>,
    },
    citation="""@inproceedings{mandlekar2019scaling,
              title={Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity},
              author={Mandlekar, Ajay and Booher, Jonathan and Spero, Max and Tung, Albert and Gupta, Anchit and Zhu, Yuke and Garg, Animesh and Savarese, Silvio and Fei-Fei, Li},
              booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
              pages={1048--1055},
              year={2019},
              organization={IEEE}
            }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="nyu_door_opening_surprising_effectiveness"> nyu_door_opening_surprising_effectiveness </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_door_opening_surprising_effectiveness/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_door_opening_surprising_effectiveness/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> open door </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_door_opening_surprising_effectiveness/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_door_opening_surprising_effectiveness',
    full_name='nyu_door_opening_surprising_effectiveness/0.1.0',
    description="""
    Hello robot opening cabinets, microwaves etc
    """,
    homepage='https://jyopari.github.io/VINN/',
    data_path='gs://gresearch/robotics/nyu_door_opening_surprising_effectiveness/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=7.12 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(720, 960, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=49, num_shards=8>,
        'train': <SplitInfo num_examples=435, num_shards=64>,
    },
    citation="""@misc{pari2021surprising,
        title={The Surprising Effectiveness of Representation Learning for Visual Imitation}, 
        author={Jyothish Pari and Nur Muhammad Shafiullah and Sridhar Pandian Arunachalam and Lerrel Pinto},
        year={2021},
        eprint={2112.01511},
        archivePrefix={arXiv},
        primaryClass={cs.RO}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="viola"> viola </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> eye_in_hand_rgb </th><th> agentview_rgb </th></tr>
<tr><td> pick up the bowl from the lazy Susan and put bowl on the plate </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode0_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode0_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> arrange plate and fork </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode1_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode1_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> arrange plate and fork </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode2_eye_in_hand_rgb.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="viola/episode2_agentview_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='viola',
    full_name='viola/0.1.0',
    description="""
    Franka robot interacting with stylized kitchen tasks
    """,
    homepage='https://ut-austin-rpl.github.io/VIOLA/',
    data_path='gs://gresearch/robotics/viola/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=10.40 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'agentview_rgb': Image(shape=(224, 224, 3), dtype=uint8),
                'ee_states': Tensor(shape=(16,), dtype=float32),
                'eye_in_hand_rgb': Image(shape=(224, 224, 3), dtype=uint8),
                'gripper_states': Tensor(shape=(1,), dtype=float32),
                'joint_states': Tensor(shape=(7,), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=15, num_shards=7>,
        'train': <SplitInfo num_examples=135, num_shards=81>,
    },
    citation="""@article{zhu2022viola,
      title={VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors},
      author={Zhu, Yifeng and Joshi, Abhishek and Stone, Peter and Zhu, Yuke},
      journal={6th Annual Conference on Robot Learning (CoRL)},
      year={2022}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_autolab_ur5"> berkeley_autolab_ur5 </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> hand_image </th><th> image </th></tr>
<tr><td> sweep the green cloth to the left side of the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> sweep the green cloth to the left side of the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> sweep the green cloth to the left side of the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_autolab_ur5/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_autolab_ur5',
    full_name='berkeley_autolab_ur5/0.1.0',
    description="""
    UR5 performing cloth manipulation, pick place etc tasks
    """,
    homepage='https://sites.google.com/view/berkeley-ur5/home',
    data_path='gs://gresearch/robotics/berkeley_autolab_ur5/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=76.39 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'image_with_depth': Image(shape=(480, 640, 1), dtype=float32),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(15,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=104, num_shards=50>,
        'train': <SplitInfo num_examples=896, num_shards=412>,
    },
    citation="""@misc{BerkeleyUR5Website,
      title = {Berkeley {UR5} Demonstration Dataset},
      author = {Lawrence Yunliang Chen and Simeon Adebola and Ken Goldberg},
      howpublished = {https://sites.google.com/view/berkeley-ur5/home},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="toto"> toto </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="toto/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="toto/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> pour </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="toto/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='toto',
    full_name='toto/0.1.0',
    description="""
    Franka scooping and pouring tasks
    """,
    homepage='https://toto-benchmark.org/',
    data_path='gs://gresearch/robotics/toto/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=127.66 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'open_gripper': bool,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=101, num_shards=52>,
        'train': <SplitInfo num_examples=902, num_shards=415>,
    },
    citation="""@inproceedings{zhou2023train,
      author={Zhou, Gaoyue and Dean, Victoria and Srirama, Mohan Kumar and Rajeswaran, Aravind and Pari, Jyothish and Hatch, Kyle and Jain, Aryan and Yu, Tianhe and Abbeel, Pieter and Pinto, Lerrel and Finn, Chelsea and Gupta, Abhinav},
      booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
      title={Train Offline, Test Online: A Real Robot Learning Benchmark}, 
      year={2023},
     }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="language_table"> language_table </h2>
    <table class="display_table">
<tr><th> rgb </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="language_table/episode0_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="language_table/episode1_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="language_table/episode2_rgb.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='language_table',
    full_name='language_table/0.0.1',
    description="""
    
    """,
    homepage='https://www.tensorflow.org/datasets/catalog/language_table',
    data_path='gs://gresearch/robotics/language_table/0.0.1',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=399.23 GiB,
    features=FeaturesDict({
        'episode_id': string,
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'effector_target_translation': Tensor(shape=(2,), dtype=float32),
                'effector_translation': Tensor(shape=(2,), dtype=float32),
                'instruction': Tensor(shape=(512,), dtype=int32),
                'rgb': Image(shape=(360, 640, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=442226, num_shards=500>,
    },
    citation="""""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="columbia_cairlab_pusht_real"> columbia_cairlab_pusht_real </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> wrist_image </th><th> image </th></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue). Both observation and control frequencies are 10Hz. </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="columbia_cairlab_pusht_real/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='columbia_cairlab_pusht_real',
    full_name='columbia_cairlab_pusht_real/0.1.0',
    description="""
    UR5 planar pushing tasks
    """,
    homepage='https://github.com/columbia-ai-robotics/diffusion_policy',
    data_path='gs://gresearch/robotics/columbia_cairlab_pusht_real/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=2.80 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'gripper_closedness_action': float32,
                'rotation_delta': Tensor(shape=(3,), dtype=float32),
                'terminate_episode': float32,
                'world_vector': Tensor(shape=(3,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'robot_state': Tensor(shape=(2,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'test': <SplitInfo num_examples=14, num_shards=4>,
        'train': <SplitInfo num_examples=122, num_shards=32>,
    },
    citation="""@inproceedings{chi2023diffusionpolicy,
    	title={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
    	author={Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
    	booktitle={Proceedings of Robotics: Science and Systems (RSS)},
    	year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="stanford_kuka_multimodal_dataset_converted_externally_to_rlds"> stanford_kuka_multimodal_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_kuka_multimodal_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_kuka_multimodal_dataset_converted_externally_to_rlds',
    full_name='stanford_kuka_multimodal_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Kuka iiwa peg insertion with force feedback
    """,
    homepage='https://sites.google.com/view/visionandtouch',
    data_path='gs://gresearch/robotics/stanford_kuka_multimodal_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=31.98 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'contact': Tensor(shape=(50,), dtype=float32),
                'depth_image': Tensor(shape=(128, 128, 1), dtype=float32),
                'ee_forces_continuous': Tensor(shape=(50, 6), dtype=float32),
                'ee_orientation': Tensor(shape=(4,), dtype=float32),
                'ee_orientation_vel': Tensor(shape=(3,), dtype=float32),
                'ee_position': Tensor(shape=(3,), dtype=float32),
                'ee_vel': Tensor(shape=(3,), dtype=float32),
                'ee_yaw': Tensor(shape=(4,), dtype=float32),
                'ee_yaw_delta': Tensor(shape=(4,), dtype=float32),
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
                'joint_vel': Tensor(shape=(7,), dtype=float32),
                'optical_flow': Tensor(shape=(128, 128, 2), dtype=float32),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=3000, num_shards=256>,
    },
    citation="""@inproceedings{lee2019icra,
      title={Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
      author={Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and  Garg, Animesh and Bohg, Jeannette},
      booktitle={2019 IEEE International Conference on Robotics and Automation (ICRA)},
      year={2019},
      url={https://arxiv.org/abs/1810.10191}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="nyu_rot_dataset_converted_externally_to_rlds"> nyu_rot_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_rot_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_rot_dataset_converted_externally_to_rlds',
    full_name='nyu_rot_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm short-horizon table-top tasks
    """,
    homepage='https://rot-robot.github.io/',
    data_path='gs://gresearch/robotics/nyu_rot_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=5.33 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(84, 84, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=14, num_shards=1>,
    },
    citation="""@inproceedings{haldar2023watch,
      title={Watch and match: Supercharging imitation with regularized optimal transport},
      author={Haldar, Siddhant and Mathur, Vaibhav and Yarats, Denis and Pinto, Lerrel},
      booktitle={Conference on Robot Learning},
      pages={32--43},
      year={2023},
      organization={PMLR}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="stanford_hydra_dataset_converted_externally_to_rlds"> stanford_hydra_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> wrist_image </th><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_hydra_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_hydra_dataset_converted_externally_to_rlds',
    full_name='stanford_hydra_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka solving long-horizon tasks
    """,
    homepage='https://sites.google.com/view/hydra-il-2023',
    data_path='gs://gresearch/robotics/stanford_hydra_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=72.48 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_dense': Scalar(shape=(), dtype=bool),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'state': Tensor(shape=(27,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=570, num_shards=350>,
    },
    citation="""@article{belkhale2023hydra,
     title={HYDRA: Hybrid Robot Actions for Imitation Learning},
     author={Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
     journal={arxiv},
     year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="austin_buds_dataset_converted_externally_to_rlds"> austin_buds_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> wrist_image </th><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_buds_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_buds_dataset_converted_externally_to_rlds',
    full_name='austin_buds_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka stylized kitchen tasks
    """,
    homepage='https://ut-austin-rpl.github.io/rpl-BUDS/',
    data_path='gs://gresearch/robotics/austin_buds_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.49 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(24,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=50, num_shards=16>,
    },
    citation="""@article{zhu2022bottom,
      title={Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation},
      author={Zhu, Yifeng and Stone, Peter and Zhu, Yuke},
      journal={IEEE Robotics and Automation Letters},
      volume={7},
      number={2},
      pages={4126--4133},
      year={2022},
      publisher={IEEE}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="nyu_franka_play_dataset_converted_externally_to_rlds"> nyu_franka_play_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> image_additional_view </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode0_image_additional_view.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode1_image_additional_view.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="nyu_franka_play_dataset_converted_externally_to_rlds/episode2_image_additional_view.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='nyu_franka_play_dataset_converted_externally_to_rlds',
    full_name='nyu_franka_play_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka interacting with toy kitchens
    """,
    homepage='https://play-to-policy.github.io/',
    data_path='gs://gresearch/robotics/nyu_franka_play_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=5.18 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(15,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth': Tensor(shape=(128, 128, 1), dtype=int32),
                'depth_additional_view': Tensor(shape=(128, 128, 1), dtype=int32),
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'image_additional_view': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=365, num_shards=32>,
        'val': <SplitInfo num_examples=91, num_shards=16>,
    },
    citation="""@article{cui2022play,
      title   = {From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data},
      author  = {Cui, Zichen Jeff and Wang, Yibin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
      journal = {arXiv preprint arXiv:2210.10047},
      year    = {2022}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="maniskill_dataset_converted_externally_to_rlds"> maniskill_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> wrist_image </th><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="maniskill_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='maniskill_dataset_converted_externally_to_rlds',
    full_name='maniskill_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Simulated Franka performing various manipulation tasks
    """,
    homepage='https://github.com/haosulab/ManiSkill2',
    data_path='gs://gresearch/robotics/maniskill_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=151.05 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'episode_id': Text(shape=(), dtype=string),
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'base_pose': Tensor(shape=(7,), dtype=float32),
                'depth': Image(shape=(256, 256, 1), dtype=uint16),
                'image': Image(shape=(256, 256, 3), dtype=uint8),
                'main_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32),
                'main_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32),
                'main_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32),
                'state': Tensor(shape=(18,), dtype=float32),
                'target_object_or_part_final_pose': Tensor(shape=(7,), dtype=float32),
                'target_object_or_part_final_pose_valid': Tensor(shape=(7,), dtype=uint8),
                'target_object_or_part_initial_pose': Tensor(shape=(7,), dtype=float32),
                'target_object_or_part_initial_pose_valid': Tensor(shape=(7,), dtype=uint8),
                'tcp_pose': Tensor(shape=(7,), dtype=float32),
                'wrist_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32),
                'wrist_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32),
                'wrist_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32),
                'wrist_depth': Image(shape=(256, 256, 1), dtype=uint16),
                'wrist_image': Image(shape=(256, 256, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=30213, num_shards=1024>,
    },
    citation="""@inproceedings{gu2023maniskill2,
      title={ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills},
      author={Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and Yuan, Xiaodi and Xie, Pengwei and Huang, Zhiao and Chen, Rui and Su, Hao},
      booktitle={International Conference on Learning Representations},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="cmu_franka_exploration_dataset_converted_externally_to_rlds"> cmu_franka_exploration_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> highres_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode0_highres_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode1_highres_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_franka_exploration_dataset_converted_externally_to_rlds/episode2_highres_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_franka_exploration_dataset_converted_externally_to_rlds',
    full_name='cmu_franka_exploration_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka exploring toy kitchens
    """,
    homepage='https://human-world-model.github.io/',
    data_path='gs://gresearch/robotics/cmu_franka_exploration_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=602.24 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'highres_image': Image(shape=(480, 640, 3), dtype=uint8),
                'image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
            'structured_action': Tensor(shape=(8,), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=199, num_shards=8>,
    },
    citation="""@inproceedings{mendonca2023structured,
                  title={Structured World Models from Human Videos},
                  author={Mendonca, Russell  and Bahl, Shikhar and Pathak, Deepak},
                  journal={RSS},
                  year={2023}
                }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="ucsd_kitchen_dataset_converted_externally_to_rlds"> ucsd_kitchen_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_kitchen_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='ucsd_kitchen_dataset_converted_externally_to_rlds',
    full_name='ucsd_kitchen_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm interacting with different toy kitchens
    """,
    homepage=' ',
    data_path='gs://gresearch/robotics/ucsd_kitchen_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.33 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=150, num_shards=16>,
    },
    citation="""@ARTICLE{ucsd_kitchens,
      author = {Ge Yan, Kris Wu, and Xiaolong Wang},
      title = {{ucsd kitchens Dataset}},
      year = {2023},
      month = {August}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="ucsd_pick_and_place_dataset_converted_externally_to_rlds"> ucsd_pick_and_place_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="ucsd_pick_and_place_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='ucsd_pick_and_place_dataset_converted_externally_to_rlds',
    full_name='ucsd_pick_and_place_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    xArm picking and placing objects with distractors
    """,
    homepage='https://owmcorl.github.io',
    data_path='gs://gresearch/robotics/ucsd_pick_and_place_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=3.53 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'disclaimer': Text(shape=(), dtype=string),
            'file_path': Text(shape=(), dtype=string),
            'n_transitions': Scalar(shape=(), dtype=int32),
            'success': Scalar(shape=(), dtype=bool),
            'success_labeled_by': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=1355, num_shards=32>,
    },
    citation="""@preprint{Feng2023Finetuning,
    	title={Finetuning Offline World Models in the Real World},
    	author={Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang},
    	year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="austin_sailor_dataset_converted_externally_to_rlds"> austin_sailor_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> wrist_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sailor_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_sailor_dataset_converted_externally_to_rlds',
    full_name='austin_sailor_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka tablesetting tasks
    """,
    homepage='https://ut-austin-rpl.github.io/sailor/',
    data_path='gs://gresearch/robotics/austin_sailor_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=18.85 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
                'state_ee': Tensor(shape=(16,), dtype=float32),
                'state_gripper': Tensor(shape=(1,), dtype=float32),
                'state_joint': Tensor(shape=(7,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=240, num_shards=109>,
    },
    citation="""@inproceedings{nasiriany2022sailor,
          title={Learning and Retrieval from Prior Data for Skill-based Imitation Learning},
          author={Soroush Nasiriany and Tian Gao and Ajay Mandlekar and Yuke Zhu},
          booktitle={Conference on Robot Learning (CoRL)},
          year={2022}
        }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="austin_sirius_dataset_converted_externally_to_rlds"> austin_sirius_dataset_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> wrist_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="austin_sirius_dataset_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='austin_sirius_dataset_converted_externally_to_rlds',
    full_name='austin_sirius_dataset_converted_externally_to_rlds/0.1.0',
    description="""
    Franka tabletop manipulation tasks
    """,
    homepage='https://ut-austin-rpl.github.io/sirius/',
    data_path='gs://gresearch/robotics/austin_sirius_dataset_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=6.55 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'action_mode': Tensor(shape=(1,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'intv_label': Tensor(shape=(1,), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(84, 84, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
                'state_ee': Tensor(shape=(16,), dtype=float32),
                'state_gripper': Tensor(shape=(1,), dtype=float32),
                'state_joint': Tensor(shape=(7,), dtype=float32),
                'wrist_image': Image(shape=(84, 84, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=559, num_shards=64>,
    },
    citation="""@inproceedings{liu2022robot,
        title = {Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment},
        author = {Huihan Liu and Soroush Nasiriany and Lance Zhang and Zhiyao Bao and Yuke Zhu},
        booktitle = {Robotics: Science and Systems (RSS)},
        year = {2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="bc_z"> bc_z </h2>
    <table class="display_table">
<tr><th> natural_language_instruction </th><th> image </th></tr>
<tr><td> place the pepper in the ceramic bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bc_z/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> place the bottle in the ceramic bowl </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bc_z/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> push the eraser across the table </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="bc_z/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='bc_z',
    full_name='bc_z/0.1.0',
    description="""
    Teleoped Google robot doing mostly pick-place from a table
    """,
    homepage='https://www.kaggle.com/datasets/google/bc-z-robot/discussion/309201',
    data_path='gs://gresearch/robotics/bc_z/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=80.54 GiB,
    features=FeaturesDict({
        'steps': Dataset({
            'action': FeaturesDict({
                'future/axis_angle_residual': Tensor(shape=(30,), dtype=float32),
                'future/target_close': Tensor(shape=(10,), dtype=int64),
                'future/xyz_residual': Tensor(shape=(30,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'observation': FeaturesDict({
                'episode_success': float32,
                'image': Image(shape=(171, 213, 3), dtype=uint8),
                'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
                'natural_language_instruction': string,
                'present/autonomous': int64,
                'present/axis_angle': Tensor(shape=(3,), dtype=float32),
                'present/intervention': int64,
                'present/sensed_close': Tensor(shape=(1,), dtype=float32),
                'present/xyz': Tensor(shape=(3,), dtype=float32),
                'sequence_length': int64,
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=39350, num_shards=1024>,
        'val': <SplitInfo num_examples=3914, num_shards=64>,
    },
    citation="""@inproceedings{jang2021bc,
    title={{BC}-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},
    author={Eric Jang and Alex Irpan and Mohi Khansari and Daniel Kappler and Frederik Ebert and Corey Lynch and Sergey Levine and Chelsea Finn},
    booktitle={5th Annual Conference on Robot Learning},
    year={2021},
    url={https://openreview.net/forum?id=8kbp23tSGYv}}""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="usc_cloth_sim_converted_externally_to_rlds"> usc_cloth_sim_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="usc_cloth_sim_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='usc_cloth_sim_converted_externally_to_rlds',
    full_name='usc_cloth_sim_converted_externally_to_rlds/0.1.0',
    description="""
    Franka cloth interaction tasks
    """,
    homepage='https://uscresl.github.io/dmfd/',
    data_path='gs://gresearch/robotics/usc_cloth_sim_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=254.52 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(4,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(32, 32, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=800, num_shards=2>,
        'val': <SplitInfo num_examples=200, num_shards=1>,
    },
    citation="""@article{salhotra2022dmfd,
        author={Salhotra, Gautam and Liu, I-Chun Arthur and Dominguez-Kuhne, Marcus and Sukhatme, Gaurav S.},
        journal={IEEE Robotics and Automation Letters},
        title={Learning Deformable Object Manipulation From Expert Demonstrations},
        year={2022},
        volume={7},
        number={4},
        pages={8775-8782},
        doi={10.1109/LRA.2022.3187843}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utokyo_pr2_opening_fridge_converted_externally_to_rlds"> utokyo_pr2_opening_fridge_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_opening_fridge_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_pr2_opening_fridge_converted_externally_to_rlds',
    full_name='utokyo_pr2_opening_fridge_converted_externally_to_rlds/0.1.0',
    description="""
    PR2 opening fridge doors
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_pr2_opening_fridge_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=360.57 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=64, num_shards=4>,
        'val': <SplitInfo num_examples=16, num_shards=1>,
    },
    citation="""@misc{oh2023pr2utokyodatasets,
      author={Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},
      title={X-Embodiment U-Tokyo PR2 Datasets},
      year={2023},
      url={https://github.com/ojh6404/rlds_dataset_builder},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds"> utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds',
    full_name='utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/0.1.0',
    description="""
    tbd
    """,
    homepage='tbd',
    data_path='gs://gresearch/robotics/utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=829.37 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=192, num_shards=8>,
        'val': <SplitInfo num_examples=48, num_shards=2>,
    },
    citation="""tbd""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utokyo_saytap_converted_externally_to_rlds"> utokyo_saytap_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> wrist_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_saytap_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_saytap_converted_externally_to_rlds',
    full_name='utokyo_saytap_converted_externally_to_rlds/0.1.0',
    description="""
    A1 walking, no RGB
    """,
    homepage='https://saytap.github.io/',
    data_path='gs://gresearch/robotics/utokyo_saytap_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=55.34 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(12,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'desired_pattern': Tensor(shape=(4, 5), dtype=bool),
                'desired_vel': Tensor(shape=(3,), dtype=float32),
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'prev_act': Tensor(shape=(12,), dtype=float32),
                'proj_grav_vec': Tensor(shape=(3,), dtype=float32),
                'state': Tensor(shape=(30,), dtype=float32),
                'wrist_image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=20, num_shards=1>,
    },
    citation="""@article{saytap2023,
      author = {Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and
    Tatsuya Harada},
      title  = {SayTap: Language to Quadrupedal Locomotion},
      eprint = {arXiv:2306.07580},
      url    = {https://saytap.github.io},
      note   = "{https://saytap.github.io}",
      year   = {2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utokyo_xarm_pick_and_place_converted_externally_to_rlds"> utokyo_xarm_pick_and_place_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> image2 </th><th> hand_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_image2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_pick_and_place_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_xarm_pick_and_place_converted_externally_to_rlds',
    full_name='utokyo_xarm_pick_and_place_converted_externally_to_rlds/0.1.0',
    description="""
    xArm picking and placing objects
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_xarm_pick_and_place_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.29 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_pose': Tensor(shape=(6,), dtype=float32),
                'hand_image': Image(shape=(224, 224, 3), dtype=uint8),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'image2': Image(shape=(224, 224, 3), dtype=uint8),
                'joint_state': Tensor(shape=(14,), dtype=float32),
                'joint_trajectory': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=92, num_shards=16>,
        'val': <SplitInfo num_examples=10, num_shards=1>,
    },
    citation="""@misc{matsushima2023weblab,
      title={Weblab xArm Dataset},
      author={Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utokyo_xarm_bimanual_converted_externally_to_rlds"> utokyo_xarm_bimanual_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utokyo_xarm_bimanual_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utokyo_xarm_bimanual_converted_externally_to_rlds',
    full_name='utokyo_xarm_bimanual_converted_externally_to_rlds/0.1.0',
    description="""
    xArm bimanual setup folding towel
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/utokyo_xarm_bimanual_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=138.44 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(14,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'action_l': Tensor(shape=(7,), dtype=float32),
                'action_r': Tensor(shape=(7,), dtype=float32),
                'image': Image(shape=(256, 256, 3), dtype=uint8),
                'pose_l': Tensor(shape=(6,), dtype=float32),
                'pose_r': Tensor(shape=(6,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=64, num_shards=1>,
        'val': <SplitInfo num_examples=6, num_shards=1>,
    },
    citation="""@misc{matsushima2023weblab,
      title={Weblab xArm Dataset},
      author={Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="robo_net"> robo_net </h2>
    <table class="display_table">
<tr><th> image1 </th><th> image </th><th> image2 </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode0_image1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode0_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode1_image1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode1_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode2_image1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="robo_net/episode2_image2.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='robo_net',
    full_name='robo_net/1.0.0',
    description="""
    This is an excerpt of the [RoboNet](https://github.com/SudeepDasari/RoboNet) dataset.
    
    
    Data from 5 robots randomly interacting with a bin using the AutoGrasp primitive. The action/state space is shared across all robots, and camera observations were taken using 3 synced camera images.
    """,
    homepage='https://www.tensorflow.org/datasets/catalog/robo_net',
    data_path='gs://gresearch/robotics/robo_net/1.0.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=799.91 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
            'robot': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(5,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': Scalar(shape=(), dtype=bool),
            'is_last': Scalar(shape=(), dtype=bool),
            'is_terminal': Scalar(shape=(), dtype=bool),
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(240, 320, 3), dtype=uint8),
                'image1': Image(shape=(240, 320, 3), dtype=uint8),
                'image2': Image(shape=(240, 320, 3), dtype=uint8),
                'state': Tensor(shape=(5,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=82775, num_shards=1024>,
    },
    citation="""@inproceedings{dasari2019robonet,
        title={RoboNet: Large-Scale Multi-Robot Learning},
        author={Sudeep Dasari and Frederik Ebert and Stephen Tian and Suraj Nair and Bernadette Bucher and Karl Schmeckpeper and Siddharth Singh and Sergey Levine and Chelsea Finn},
        year={2019},
        eprint={1910.11215},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        booktitle={CoRL 2019: Volume 100 Proceedings of Machine Learning Research}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_mvp_converted_externally_to_rlds"> berkeley_mvp_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> hand_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_mvp_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_mvp_converted_externally_to_rlds',
    full_name='berkeley_mvp_converted_externally_to_rlds/0.1.0',
    description="""
    xArm performing 6 manipulation tasks
    """,
    homepage='https://arxiv.org/abs/2203.06173',
    data_path='gs://gresearch/robotics/berkeley_mvp_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=12.34 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'gripper': Scalar(shape=(), dtype=bool),
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
                'pose': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=480, num_shards=124>,
    },
    citation="""@InProceedings{Radosavovic2022,
      title = {Real-World Robot Learning with Masked Visual Pre-training},
      author = {Ilija Radosavovic and Tete Xiao and Stephen James and Pieter Abbeel and Jitendra Malik and Trevor Darrell},
      booktitle = {CoRL},
      year = {2022}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_rpt_converted_externally_to_rlds"> berkeley_rpt_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> hand_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode0_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode1_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_rpt_converted_externally_to_rlds/episode2_hand_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_rpt_converted_externally_to_rlds',
    full_name='berkeley_rpt_converted_externally_to_rlds/0.1.0',
    description="""
    Franka performing tabletop pick place tasks
    """,
    homepage='https://arxiv.org/abs/2306.10007',
    data_path='gs://gresearch/robotics/berkeley_rpt_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=40.64 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'gripper': Scalar(shape=(), dtype=bool),
                'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
                'joint_pos': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=908, num_shards=441>,
    },
    citation="""@article{Radosavovic2023,
      title={Robot Learning with Sensorimotor Pre-training},
      author={Ilija Radosavovic and Baifeng Shi and Letian Fu and Ken Goldberg and Trevor Darrell and Jitendra Malik},
      year={2023},
      journal={arXiv:2306.10007}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="kaist_nonprehensile_converted_externally_to_rlds"> kaist_nonprehensile_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="kaist_nonprehensile_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='kaist_nonprehensile_converted_externally_to_rlds',
    full_name='kaist_nonprehensile_converted_externally_to_rlds/0.1.0',
    description="""
    Franka manipulating ungraspable objects
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/kaist_nonprehensile_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=11.71 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(20,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'partial_pointcloud': Tensor(shape=(512, 3), dtype=float32),
                'state': Tensor(shape=(21,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=201, num_shards=101>,
    },
    citation="""@article{kimpre,
      title={Pre-and post-contact policy decomposition for non-prehensile manipulation with zero-shot sim-to-real transfer},
      author={Kim, Minchan and Han, Junhyek and Kim, Jaehyung and Kim, Beomjoon},
      booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="stanford_mask_vit_converted_externally_to_rlds"> stanford_mask_vit_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_mask_vit_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_mask_vit_converted_externally_to_rlds',
    full_name='stanford_mask_vit_converted_externally_to_rlds/0.1.0',
    description="""
    Sawyer pushing and picking objects in a bin
    """,
    homepage='https://arxiv.org/abs/2206.11894',
    data_path='gs://gresearch/robotics/stanford_mask_vit_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=76.17 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(5,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_pose': Tensor(shape=(5,), dtype=float32),
                'finger_sensors': Tensor(shape=(1,), dtype=float32),
                'high_bound': Tensor(shape=(5,), dtype=float32),
                'image': Image(shape=(480, 480, 3), dtype=uint8),
                'low_bound': Tensor(shape=(5,), dtype=float32),
                'state': Tensor(shape=(15,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=9109, num_shards=1023>,
        'val': <SplitInfo num_examples=91, num_shards=8>,
    },
    citation="""@inproceedings{gupta2022maskvit,
      title={MaskViT: Masked Visual Pre-Training for Video Prediction},
      author={Agrim Gupta and Stephen Tian and Yunzhi Zhang and Jiajun Wu and Roberto Martín-Martín and Li Fei-Fei},
      booktitle={International Conference on Learning Representations},
      year={2022}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="tokyo_u_lsmo_converted_externally_to_rlds"> tokyo_u_lsmo_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="tokyo_u_lsmo_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='tokyo_u_lsmo_converted_externally_to_rlds',
    full_name='tokyo_u_lsmo_converted_externally_to_rlds/0.1.0',
    description="""
    motion planning trajectory of pick place tasks
    """,
    homepage='https://journals.sagepub.com/doi/full/10.1177/02783649211044405',
    data_path='gs://gresearch/robotics/tokyo_u_lsmo_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=335.71 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 120, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=50, num_shards=4>,
    },
    citation="""@Article{Osa22,
      author  = {Takayuki Osa},
      journal = {The International Journal of Robotics Research},
      title   = {Motion Planning by Learning the Solution Manifold in Trajectory Optimization},
      year    = {2022},
      number  = {3},
      pages   = {291--311},
      volume  = {41},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="dlr_sara_pour_converted_externally_to_rlds"> dlr_sara_pour_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_pour_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_sara_pour_converted_externally_to_rlds',
    full_name='dlr_sara_pour_converted_externally_to_rlds/0.1.0',
    description="""
    pouring liquid from a bottle into a mug
    """,
    homepage='https://elib.dlr.de/193739/1/padalkar2023rlsct.pdf',
    data_path='gs://gresearch/robotics/dlr_sara_pour_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=2.92 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(6,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=100, num_shards=31>,
    },
    citation="""@inproceedings{padalkar2023guiding,
      title={Guiding Reinforcement Learning with Shared Control Templates},
      author={Padalkar, Abhishek and Quere, Gabriel and Steinmetz, Franz and Raffin, Antonin and Nieuwenhuisen, Matthias and Silv{'e}rio, Jo{\~a}o and Stulp, Freek},
      booktitle={40th IEEE International Conference on Robotics and Automation, ICRA 2023},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="dlr_sara_grid_clamp_converted_externally_to_rlds"> dlr_sara_grid_clamp_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_sara_grid_clamp_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_sara_grid_clamp_converted_externally_to_rlds',
    full_name='dlr_sara_grid_clamp_converted_externally_to_rlds/0.1.0',
    description="""
    place grid clamp onto grids on table
    """,
    homepage='https://www.researchsquare.com/article/rs-3289569/v1',
    data_path='gs://gresearch/robotics/dlr_sara_grid_clamp_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.65 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(480, 640, 3), dtype=uint8),
                'state': Tensor(shape=(12,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=107, num_shards=16>,
    },
    citation="""@article{padalkar2023guided,
      title={A guided reinforcement learning approach using shared control templates for learning manipulation skills in the real world},
      author={Padalkar, Abhishek and Quere, Gabriel and Raffin, Antonin and Silv{'e}rio, Jo{\~a}o and Stulp, Freek},
      journal={Research square preprint rs-3289569/v1},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="dlr_edan_shared_control_converted_externally_to_rlds"> dlr_edan_shared_control_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="dlr_edan_shared_control_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='dlr_edan_shared_control_converted_externally_to_rlds',
    full_name='dlr_edan_shared_control_converted_externally_to_rlds/0.1.0',
    description="""
    wheelchair with arm performing shelf pick tasks
    """,
    homepage='https://ieeexplore.ieee.org/document/9341156',
    data_path='gs://gresearch/robotics/dlr_edan_shared_control_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=3.09 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=104, num_shards=29>,
    },
    citation="""@inproceedings{vogel_edan_2020,
    	title = {EDAN - an EMG-Controlled Daily Assistant to Help People with Physical Disabilities},
    	language = {en},
    	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
    	author = {Vogel, Jörn and Hagengruber, Annette and Iskandar, Maged and Quere, Gabriel and Leipscher, Ulrike and Bustamante, Samuel and Dietrich, Alexander and Hoeppner, Hannes and Leidner, Daniel and Albu-Schäffer, Alin},
    	year = {2020}
    }
    @inproceedings{quere_shared_2020,
    	address = {Paris, France},
    	title = {Shared {Control} {Templates} for {Assistive} {Robotics}},
    	language = {en},
    	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
    	author = {Quere, Gabriel and Hagengruber, Annette and Iskandar, Maged and Bustamante, Samuel and Leidner, Daniel and Stulp, Freek and Vogel, Joern},
    	year = {2020},
    	pages = {7},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="asu_table_top_converted_externally_to_rlds"> asu_table_top_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="asu_table_top_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="asu_table_top_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="asu_table_top_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='asu_table_top_converted_externally_to_rlds',
    full_name='asu_table_top_converted_externally_to_rlds/0.1.0',
    description="""
    UR5 performing table-top pick/place/rotate tasks
    """,
    homepage='https://link.springer.com/article/10.1007/s10514-023-10129-1',
    data_path='gs://gresearch/robotics/asu_table_top_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=737.60 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'action_delta': Tensor(shape=(7,), dtype=float32),
            'action_inst': Text(shape=(), dtype=string),
            'discount': Scalar(shape=(), dtype=float32),
            'goal_object': Text(shape=(), dtype=string),
            'ground_truth_states': FeaturesDict({
                'EE': Tensor(shape=(6,), dtype=float32),
                'bottle': Tensor(shape=(6,), dtype=float32),
                'bread': Tensor(shape=(6,), dtype=float32),
                'coke': Tensor(shape=(6,), dtype=float32),
                'cube': Tensor(shape=(6,), dtype=float32),
                'milk': Tensor(shape=(6,), dtype=float32),
                'pepsi': Tensor(shape=(6,), dtype=float32),
            }),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
                'state_vel': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=110, num_shards=8>,
    },
    citation="""@inproceedings{zhou2023modularity,
      title={Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation},
      author={Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Stepputtis, Simon and Amor, Heni},
      booktitle={Conference on Robot Learning},
      pages={1684--1695},
      year={2023},
      organization={PMLR}
    }
    @article{zhou2023learning,
      title={Learning modular language-conditioned robot policies through attention},
      author={Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Ben Amor, Heni and Stepputtis, Simon},
      journal={Autonomous Robots},
      pages={1--21},
      year={2023},
      publisher={Springer}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="stanford_robocook_converted_externally_to_rlds"> stanford_robocook_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image_1 </th><th> image_3 </th><th> image_4 </th><th> image_2 </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode0_image_2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode1_image_2.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_3.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="stanford_robocook_converted_externally_to_rlds/episode2_image_2.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='stanford_robocook_converted_externally_to_rlds',
    full_name='stanford_robocook_converted_externally_to_rlds/0.1.0',
    description="""
    Franka preparing dumplings with various tools
    """,
    homepage='https://hshi74.github.io/robocook/',
    data_path='gs://gresearch/robotics/stanford_robocook_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=124.62 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'extrinsics_1': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_2': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_3': Tensor(shape=(4, 4), dtype=float32),
            'extrinsics_4': Tensor(shape=(4, 4), dtype=float32),
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth_1': Tensor(shape=(256, 256), dtype=float32),
                'depth_2': Tensor(shape=(256, 256), dtype=float32),
                'depth_3': Tensor(shape=(256, 256), dtype=float32),
                'depth_4': Tensor(shape=(256, 256), dtype=float32),
                'image_1': Image(shape=(256, 256, 3), dtype=uint8),
                'image_2': Image(shape=(256, 256, 3), dtype=uint8),
                'image_3': Image(shape=(256, 256, 3), dtype=uint8),
                'image_4': Image(shape=(256, 256, 3), dtype=uint8),
                'state': Tensor(shape=(7,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=2460, num_shards=923>,
    },
    citation="""@article{shi2023robocook,
      title={RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools},
      author={Shi, Haochen and Xu, Huazhe and Clarke, Samuel and Li, Yunzhu and Wu, Jiajun},
      journal={arXiv preprint arXiv:2306.14447},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="eth_agent_affordances"> eth_agent_affordances </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="eth_agent_affordances/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="eth_agent_affordances/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="eth_agent_affordances/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='eth_agent_affordances',
    full_name='eth_agent_affordances/0.1.0',
    description="""
    Franka opening ovens -- point cloud + proprio only
    """,
    homepage='https://ieeexplore.ieee.org/iel7/10160211/10160212/10160747.pdf',
    data_path='gs://gresearch/robotics/eth_agent_affordances/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=17.27 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
            'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(6,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=118, num_shards=53>,
    },
    citation="""@inproceedings{schiavi2023learning,
      title={Learning agent-aware affordances for closed-loop interaction with articulated objects},
      author={Schiavi, Giulio and Wulkop, Paula and Rizzi, Giuseppe and Ott, Lionel and Siegwart, Roland and Chung, Jen Jen},
      booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
      pages={5916--5922},
      year={2023},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="imperialcollege_sawyer_wrist_cam"> imperialcollege_sawyer_wrist_cam </h2>
    <table class="display_table">
<tr><th> image </th><th> wrist_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="imperialcollege_sawyer_wrist_cam/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='imperialcollege_sawyer_wrist_cam',
    full_name='imperialcollege_sawyer_wrist_cam/0.1.0',
    description="""
    Sawyer performing table top manipulation
    """,
    homepage='--',
    data_path='gs://gresearch/robotics/imperialcollege_sawyer_wrist_cam/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=81.87 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 64, 3), dtype=uint8),
                'state': Tensor(shape=(1,), dtype=float32),
                'wrist_image': Image(shape=(64, 64, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=170, num_shards=1>,
    },
    citation="""--""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="iamlab_cmu_pickup_insert_converted_externally_to_rlds"> iamlab_cmu_pickup_insert_converted_externally_to_rlds </h2>
    <table class="display_table">
<tr><th> image </th><th> wrist_image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode0_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode1_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode2_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="iamlab_cmu_pickup_insert_converted_externally_to_rlds/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='iamlab_cmu_pickup_insert_converted_externally_to_rlds',
    full_name='iamlab_cmu_pickup_insert_converted_externally_to_rlds/0.1.0',
    description="""
    Franka picking objects and insertion tasks
    """,
    homepage='https://openreview.net/forum?id=WuBv9-IGDUA',
    data_path='gs://gresearch/robotics/iamlab_cmu_pickup_insert_converted_externally_to_rlds/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=50.29 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(20,), dtype=float32),
                'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=631, num_shards=369>,
    },
    citation="""@inproceedings{
    saxena2023multiresolution,
    title={Multi-Resolution Sensing for Real-Time Control with Vision-Language Models},
    author={Saumya Saxena and Mohit Sharma and Oliver Kroemer},
    booktitle={7th Annual Conference on Robot Learning},
    year={2023},
    url={https://openreview.net/forum?id=WuBv9-IGDUA}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="uiuc_d3field"> uiuc_d3field </h2>
    <table class="display_table">
<tr><th> image_1 </th><th> image_4 </th><th> image_2 </th><th> image_3 </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode0_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode0_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode0_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode0_image_3.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode1_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode1_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode1_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode1_image_3.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode2_image_1.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode2_image_4.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode2_image_2.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="uiuc_d3field/episode2_image_3.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='uiuc_d3field',
    full_name='uiuc_d3field/0.1.0',
    description="""
    Organizing office desk, utensils etc
    """,
    homepage='https://robopil.github.io/d3fields/',
    data_path='gs://gresearch/robotics/uiuc_d3field/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=15.82 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(3,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'depth_1': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_2': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_3': Image(shape=(360, 640, 1), dtype=uint16),
                'depth_4': Image(shape=(360, 640, 1), dtype=uint16),
                'image_1': Image(shape=(360, 640, 3), dtype=uint8),
                'image_2': Image(shape=(360, 640, 3), dtype=uint8),
                'image_3': Image(shape=(360, 640, 3), dtype=uint8),
                'image_4': Image(shape=(360, 640, 3), dtype=uint8),
                'state': Tensor(shape=(4, 4), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=192, num_shards=98>,
    },
    citation="""@article{wang2023d3field,
      title={D^3Field: Dynamic 3D Descriptor Fields for Generalizable Robotic Manipulation}, 
      author={Wang, Yixuan and Li, Zhuoran and Zhang, Mingtong and Driggs-Campbell, Katherine and Wu, Jiajun and Fei-Fei, Li and Li, Yunzhu},
      journal={arXiv preprint arXiv:},
      year={2023},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="utaustin_mutex"> utaustin_mutex </h2>
    <table class="display_table">
<tr><th> wrist_image </th><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="utaustin_mutex/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='utaustin_mutex',
    full_name='utaustin_mutex/0.1.0',
    description="""
    Diverse household manipulation tasks
    """,
    homepage='https://ut-austin-rpl.github.io/MUTEX/',
    data_path='gs://gresearch/robotics/utaustin_mutex/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=20.79 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(7,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(24,), dtype=float32),
                'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=1500, num_shards=256>,
    },
    citation="""@inproceedings{
        shah2023mutex,
        title={{MUTEX}: Learning Unified Policies from Multimodal Task Specifications},
        author={Rutav Shah and Roberto Mart{'\i}n-Mart{'\i}n and Yuke Zhu},
        booktitle={7th Annual Conference on Robot Learning},
        year={2023},
        url={https://openreview.net/forum?id=PwqiqaaEzJ}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_fanuc_manipulation"> berkeley_fanuc_manipulation </h2>
    <table class="display_table">
<tr><th> wrist_image </th><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode0_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode1_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode2_wrist_image.mp4" type="video/mp4">
    </video>
     </td><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_fanuc_manipulation/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_fanuc_manipulation',
    full_name='berkeley_fanuc_manipulation/0.1.0',
    description="""
    Fanuc robot performing various manipulation tasks
    """,
    homepage='https://sites.google.com/berkeley.edu/fanuc-manipulation',
    data_path='gs://gresearch/robotics/berkeley_fanuc_manipulation/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=8.85 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(6,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'end_effector_state': Tensor(shape=(7,), dtype=float32),
                'image': Image(shape=(224, 224, 3), dtype=uint8),
                'state': Tensor(shape=(13,), dtype=float32),
                'wrist_image': Image(shape=(224, 224, 3), dtype=uint8),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=415, num_shards=124>,
    },
    citation="""@article{fanuc_manipulation2023,
      title={Fanuc Manipulation: A Dataset for Learning-based Manipulation with FANUC Mate 200iD Robot},
      author={Zhu, Xinghao and Tian, Ran and Xu, Chenfeng and Ding, Mingyu and Zhan, Wei and Tomizuka, Masayoshi},
      year={2023},
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="cmu_play_fusion"> cmu_play_fusion </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_play_fusion/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_play_fusion/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_play_fusion/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_play_fusion',
    full_name='cmu_play_fusion/0.1.0',
    description="""
    The robot plays with 3 complex scenes: a grill with many cooking objects like toaster, pan, etc. It has to pick, open, place, close. It  has to set a table, move plates, cups, utensils. And it has to place dishes in the sink, dishwasher, hand cups etc.
    """,
    homepage='https://play-fusion.github.io/',
    data_path='gs://gresearch/robotics/cmu_play_fusion/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=6.68 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(9,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(8,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=576, num_shards=64>,
    },
    citation="""@inproceedings{chen2023playfusion,
      title={PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play},
      author={Chen, Lili and Bahl, Shikhar and Pathak, Deepak},
      booktitle={CoRL},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="cmu_stretch"> cmu_stretch </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_stretch/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_stretch/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="cmu_stretch/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='cmu_stretch',
    full_name='cmu_stretch/0.1.0',
    description="""
    Hello stretch robot kitchen interactions
    """,
    homepage='https://robo-affordances.github.io/',
    data_path='gs://gresearch/robotics/cmu_stretch/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=728.06 MiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(8,), dtype=float32),
            'discount': Scalar(shape=(), dtype=float32),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(128, 128, 3), dtype=uint8),
                'state': Tensor(shape=(4,), dtype=float32),
            }),
            'reward': Scalar(shape=(), dtype=float32),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=135, num_shards=8>,
    },
    citation="""@inproceedings{bahl2023affordances,
      title={Affordances from Human Videos as a Versatile Representation for Robotics},
      author={Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},
      booktitle={CVPR},
      year={2023}
    }
    @article{mendonca2023structured,
      title={Structured World Models from Human Videos},
      author={Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
      journal={CoRL},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_gnm_recon"> berkeley_gnm_recon </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_recon/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_recon/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_recon/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_recon',
    full_name='berkeley_gnm_recon/0.1.0',
    description="""
    off-road navigation
    """,
    homepage='https://sites.google.com/view/recon-robot',
    data_path='gs://gresearch/robotics/berkeley_gnm_recon/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=18.73 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 160, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=11834, num_shards=256>,
    },
    citation="""@inproceedings{
    shah2021rapid,
    title={{Rapid Exploration for Open-World Navigation with Latent Goal Models}},
    author={Dhruv Shah and Benjamin Eysenbach and Nicholas Rhinehart and Sergey Levine},
    booktitle={5th Annual Conference on Robot Learning },
    year={2021},
    url={https://openreview.net/forum?id=d_SWJhyKfVw}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_gnm_cory_hall"> berkeley_gnm_cory_hall </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_cory_hall/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_cory_hall/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_cory_hall/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_cory_hall',
    full_name='berkeley_gnm_cory_hall/0.1.0',
    description="""
    hallway navigation
    """,
    homepage='https://arxiv.org/abs/1709.10489',
    data_path='gs://gresearch/robotics/berkeley_gnm_cory_hall/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=1.39 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(64, 85, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=7331, num_shards=16>,
    },
    citation="""@inproceedings{kahn2018self,
      title={Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation},
      author={Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
      booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
      pages={5129--5136},
      year={2018},
      organization={IEEE}
    }""",
)
    </pre>
    </div>
    
    

    <div class="dataset">
    <h2 id="berkeley_gnm_sac_son"> berkeley_gnm_sac_son </h2>
    <table class="display_table">
<tr><th> image </th></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_sac_son/episode0_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_sac_son/episode1_image.mp4" type="video/mp4">
    </video>
     </td></tr>
<tr><td> 
    <video autoplay loop muted playsinline class="video-background ">
      <source src="berkeley_gnm_sac_son/episode2_image.mp4" type="video/mp4">
    </video>
     </td></tr>
 </table>

    <pre class="dataset_info">
    tfds.core.DatasetInfo(
    name='berkeley_gnm_sac_son',
    full_name='berkeley_gnm_sac_son/0.1.0',
    description="""
    office navigation
    """,
    homepage='https://sites.google.com/view/SACSoN-review',
    data_path='gs://gresearch/robotics/berkeley_gnm_sac_son/0.1.0',
    file_format=tfrecord,
    download_size=Unknown size,
    dataset_size=7.00 GiB,
    features=FeaturesDict({
        'episode_metadata': FeaturesDict({
            'file_path': Text(shape=(), dtype=string),
        }),
        'steps': Dataset({
            'action': Tensor(shape=(2,), dtype=float64),
            'action_angle': Tensor(shape=(3,), dtype=float64),
            'discount': Scalar(shape=(), dtype=float64),
            'is_first': bool,
            'is_last': bool,
            'is_terminal': bool,
            'language_embedding': Tensor(shape=(512,), dtype=float32),
            'language_instruction': Text(shape=(), dtype=string),
            'observation': FeaturesDict({
                'image': Image(shape=(120, 160, 3), dtype=uint8),
                'position': Tensor(shape=(2,), dtype=float64),
                'state': Tensor(shape=(3,), dtype=float64),
                'yaw': Tensor(shape=(1,), dtype=float64),
            }),
            'reward': Scalar(shape=(), dtype=float64),
        }),
    }),
    supervised_keys=None,
    disable_shuffling=False,
    splits={
        'train': <SplitInfo num_examples=2955, num_shards=64>,
    },
    citation="""@article{hirose2023sacson,
      title={SACSoN: Scalable Autonomous Data Collection for Social Navigation},
      author={Hirose, Noriaki and Shah, Dhruv and Sridhar, Ajay and Levine, Sergey},
      journal={arXiv preprint arXiv:2306.01874},
      year={2023}
    }""",
)
    </pre>
    </div>
    
    