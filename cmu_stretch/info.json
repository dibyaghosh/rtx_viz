{
  "repr": "tfds.core.DatasetInfo(\n    name='cmu_stretch',\n    full_name='cmu_stretch/0.1.0',\n    description=\"\"\"\n    Hello stretch robot kitchen interactions\n    \"\"\",\n    homepage='https://robo-affordances.github.io/',\n    data_path='gs://gresearch/robotics/cmu_stretch/0.1.0',\n    file_format=tfrecord,\n    download_size=Unknown size,\n    dataset_size=728.06 MiB,\n    features=FeaturesDict({\n        'episode_metadata': FeaturesDict({\n            'file_path': Text(shape=(), dtype=string),\n        }),\n        'steps': Dataset({\n            'action': Tensor(shape=(8,), dtype=float32),\n            'discount': Scalar(shape=(), dtype=float32),\n            'is_first': bool,\n            'is_last': bool,\n            'is_terminal': bool,\n            'language_embedding': Tensor(shape=(512,), dtype=float32),\n            'language_instruction': Text(shape=(), dtype=string),\n            'observation': FeaturesDict({\n                'image': Image(shape=(128, 128, 3), dtype=uint8),\n                'state': Tensor(shape=(4,), dtype=float32),\n            }),\n            'reward': Scalar(shape=(), dtype=float32),\n        }),\n    }),\n    supervised_keys=None,\n    disable_shuffling=False,\n    splits={\n        'train': <SplitInfo num_examples=135, num_shards=8>,\n    },\n    citation=\"\"\"@inproceedings{bahl2023affordances,\n      title={Affordances from Human Videos as a Versatile Representation for Robotics},\n      author={Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},\n      booktitle={CVPR},\n      year={2023}\n    }\n    @article{mendonca2023structured,\n      title={Structured World Models from Human Videos},\n      author={Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},\n      journal={CoRL},\n      year={2023}\n    }\"\"\",\n)",
  "size": "728.06 MiB",
  "n_traj": 135,
  "image_keys": {
    "image": [
      128,
      128,
      3
    ]
  },
  "depth_keys": {},
  "text_keys": []
}